import os
import aiohttp
import asyncio
from openai import AsyncOpenAI
from dotenv import load_dotenv
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool
from bs4 import BeautifulSoup
import re
from serpapi import GoogleSearch
import time

load_dotenv()

word_ai = APIRouter(tags=["wordå°ˆç”¨"])

# å®šç¾©è«‹æ±‚æ¨¡å‹
class CompanyRequest(BaseModel):
    org_name: str
    business_id: str

# ğŸ”¹ **æ­¥é©Ÿ1ï¼šä½¿ç”¨ SERPAPI æœå°‹ä¼æ¥­ç›¸é—œé€£çµ**
async def get_google_search_results(org_name: str, business_id: str, max_results=10):
    """
    ä½¿ç”¨ SERPAPI æœå°‹ä¼æ¥­çš„ç›¸é—œé€£çµï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰
    """
    def _search():
        serpapi_key = os.getenv("SERPAPI_API_KEY")
        search_query = f"{org_name} {business_id}"

        params = {
            "engine": "google",
            "q": search_query,
            "hl": "zh-TW",
            "gl": "tw",
            "api_key": serpapi_key
        }

        search = GoogleSearch(params)
        results = search.get_dict()

        if "organic_results" not in results:
            raise HTTPException(status_code=500, detail="âŒ SERPAPI æœå°‹å¤±æ•—ï¼Œæœªç²å–çµæœ")

        # **æå–å‰ N å€‹ç›¸é—œé€£çµ**
        links = []
        for result in results["organic_results"][:max_results]:
            link = result.get("link")
            if link:
                links.append(link)

        return links
    
    # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œæœç´¢
    return await run_in_threadpool(_search)

# ğŸ”¹ **æ­¥é©Ÿ2ï¼šæ“·å–ç¶²é å…§å®¹ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰**
async def extract_web_content(session: aiohttp.ClientSession, url: str):
    """
    ç•°æ­¥æ“·å–ç¶²é å…§å®¹
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }

    try:
        async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as response:
            if response.status != 200:
                print(f"âŒ HTTP {response.status} for {url}")
                return None
                
            # ç²å–éŸ¿æ‡‰å…§å®¹
            content = await response.text(encoding='utf-8', errors='ignore')
            
            # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œ BeautifulSoup è§£æï¼ˆCPU å¯†é›†å‹æ“ä½œï¼‰
            def _parse_content():
                soup = BeautifulSoup(content, "html.parser")

                # **æ“·å–ç¶²ç«™æ¨™é¡Œ**
                title_tag = soup.find("title")
                title = title_tag.text.strip() if title_tag else "æœªæ‰¾åˆ°æ¨™é¡Œ"

                # **æ“·å– meta description**
                description_tag = soup.find("meta", attrs={"name": "description"})
                description = description_tag["content"].strip() if description_tag else "æœªæ‰¾åˆ°ç¶²ç«™ç°¡ä»‹"

                # **æ“·å– meta keywords**
                keywords_tag = soup.find("meta", attrs={"name": "keywords"})
                keywords = keywords_tag["content"].strip() if keywords_tag else "æœªæ‰¾åˆ°é—œéµå­—"

                # **æ“·å–ä¸»è¦å…§å®¹ï¼ˆæœ€å¤š 10 æ®µï¼‰**
                paragraphs = [p.text.strip() for p in soup.find_all("p") if p.text.strip()]
                headings = [h.text.strip() for h in soup.find_all(["h1", "h2", "h3"]) if h.text.strip()]

                # åˆä½µæ¨™é¡Œèˆ‡å…§æ–‡
                main_content = "\n".join(headings[:5] + paragraphs[:10])

                return {
                    "url": url,
                    "title": title,
                    "description": description,
                    "keywords": keywords,
                    "content": main_content
                }
            
            return await run_in_threadpool(_parse_content)

    except Exception as e:
        print(f"âŒ æ“·å–ç¶²é å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None

# ğŸ”¹ **æ­¥é©Ÿ3ï¼šæ‰¹é‡ç•°æ­¥çˆ¬å–**
async def batch_extract_web_content(links: list):
    """
    æ‰¹é‡ç•°æ­¥çˆ¬å–ç¶²é å…§å®¹
    """
    results = []
    
    # å‰µå»º aiohttp session
    connector = aiohttp.TCPConnector(limit=5)  # é™åˆ¶åŒæ™‚é€£æ¥æ•¸
    timeout = aiohttp.ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # å‰µå»ºæ‰€æœ‰çˆ¬å–ä»»å‹™
        tasks = []
        for link in links:
            task = extract_web_content(session, link)
            tasks.append(task)
        
        # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰çˆ¬å–ä»»å‹™ï¼Œä½†é™åˆ¶åŒæ™‚åŸ·è¡Œçš„æ•¸é‡
        semaphore = asyncio.Semaphore(3)  # åŒæ™‚æœ€å¤š3å€‹è«‹æ±‚
        
        async def bounded_extract(link_task):
            async with semaphore:
                return await link_task
        
        # åŸ·è¡Œæ‰€æœ‰ä»»å‹™
        bounded_tasks = [bounded_extract(task) for task in tasks]
        results = await asyncio.gather(*bounded_tasks, return_exceptions=True)
        
        # éæ¿¾æ‰å¤±æ•—çš„çµæœ
        valid_results = [result for result in results if result is not None and not isinstance(result, Exception)]
        
    return valid_results

# ğŸ”¹ **å„ªåŒ–å¾Œçš„å–®æ¬¡ OpenAI è™•ç† - åˆä½µå…©å€‹æ­¥é©Ÿ**
async def generate_intro_and_summary_from_results_optimized(company_name: str, company_id: str, results: list):
    """å„ªåŒ–ç‰ˆæœ¬ï¼šä¸€æ¬¡æ€§éæ¿¾ä¸¦ç”Ÿæˆä¼æ¥­å‰è¨€èˆ‡ç°¡ä»‹ï¼Œæ¸›å°‘APIèª¿ç”¨æ¬¡æ•¸"""
    openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # **æ•´åˆæ‰€æœ‰ç¶²é å…§å®¹**
    all_content = ""
    for idx, result in enumerate(results):
        all_content += f"\nç¶²é {idx+1}ï¼š\næ¨™é¡Œï¼š{result['title']}\næè¿°ï¼š{result['description']}\nå…§å®¹ï¼š{result['content']}\n"
    
    # **ä¸€æ¬¡æ€§ Promptï¼šéæ¿¾ + ç”Ÿæˆ**
    combined_prompt = [
        {"role": "system", "content": """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¼æ¥­è³‡è¨Šåˆ†æå¸«ã€‚è«‹å®Œæˆä»¥ä¸‹ä»»å‹™ï¼š
1. é¦–å…ˆå¾æä¾›çš„ç¶²é å…§å®¹ä¸­ç¯©é¸å‡ºèˆ‡ç›®æ¨™ä¼æ¥­ç›´æ¥ç›¸é—œçš„è³‡è¨Š
2. åŸºæ–¼ç¯©é¸å¾Œçš„è³‡è¨Šï¼Œç”Ÿæˆä¼æ¥­å‰è¨€å’Œä¼æ¥­ç°¡ä»‹

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ï¼š
ä¼æ¥­å‰è¨€ï¼š[åœ¨é€™è£¡è¼¸å‡ºä¼æ¥­å‰è¨€å…§å®¹]

ä¼æ¥­ç°¡ä»‹ï¼š[åœ¨é€™è£¡è¼¸å‡ºä¼æ¥­ç°¡ä»‹å…§å®¹]"""},
        
        {"role": "user", "content": f"""
ä¼æ¥­åç¨±ï¼š{company_name}
ä¼æ¥­ç·¨è™Ÿï¼š{company_id}

è«‹æ ¹æ“šä»¥ä¸‹ç¶²é è³‡è¨Šï¼Œå…ˆç¯©é¸èˆ‡è©²ä¼æ¥­ç›¸é—œçš„å…§å®¹ï¼Œç„¶å¾Œç”Ÿæˆä¼æ¥­å‰è¨€èˆ‡ç°¡ä»‹ï¼š

{all_content}

ç”Ÿæˆè¦æ±‚ï¼š
ä¼æ¥­å‰è¨€ï¼šæè¿°ä¼æ¥­çš„é¡˜æ™¯ã€ä½¿å‘½ã€ç¶“ç‡Ÿç†å¿µç­‰ï¼ˆæ­£å¼ã€åš´è¬¹ï¼‰ï¼Œåƒè€ƒä»¥ä¸‹ç¯„ä¾‹æ¶æ§‹ï¼š
æœ¬æ ¡å‰µæ ¡è¿„ä»Šï¼Œæ­·ä»»æ ¡é•·éµå¾ªå‰µè¾¦äººå‰µæ ¡è·å¿—ï¼Œç¶“ç‡Ÿæ“˜ç•«ï¼Œç©æ¥µç™¼æšã€Œèª ã€å‹¤ã€æ¨¸ã€æ…ã€å‰µæ–°ã€ç²¾ç¥å½¢æˆå„ªè‰¯æ ¡é¢¨ï¼Œä¸¦ç§‰æŒã€Œå‰µæ„ã€å‹™å¯¦ã€å®è§€ã€åˆä½œã€æºé€šã€ç†±å¿±ã€çš„æ•™è‚²ç†å¿µï¼Œä»¥ç§‘æŠ€èˆ‡äººæ–‡èåŒ¯ã€å‰µæ–°èˆ‡å“è³ªä¸¦é‡ã€å°ˆæ¥­èˆ‡é€šè­˜å…¼é¡§ã€ç†è«–èˆ‡å¯¦å‹™çµåˆç‚ºä¸»è»¸ï¼Œç™¼å±•ç‚ºå¯¦å‹™åŒ–ã€è³‡è¨ŠåŒ–ã€äººæ–‡åŒ–ã€å‰µæ–°åŒ–ã€åœ‹éš›åŒ–çš„é«˜ç­‰å­¸åºœã€‚
ç‚ºæä¾›å­¸ç”Ÿå¤šå…ƒå­¸ç¿’ï¼Œæ•´åˆç›¸é—œå­¸è¡“è³‡æºï¼Œæœ¬æ ¡ç‰¹æˆç«‹é›»é€šã€å·¥ç¨‹ã€é†«è­·æš¨ç®¡ç†ä¸‰å¤§å­¸é™¢ï¼Œè—‰ç”±å„å­¸ç³»çš„åˆä½œã€å› æ‡‰ç”¢æ¥­éœ€æ±‚ï¼Œé–‹è¨­ç›¸é—œå­¸ç¨‹ï¼Œè®“å­¸ç”Ÿé€éè·¨é ˜åŸŸå­¸ç¿’ï¼Œæå‡å°ˆæ¥­çŸ¥èƒ½èˆ‡è·å ´ç«¶çˆ­åŠ›ã€‚
æœ¬æ ¡ç©æ¥µæå‡æ•™å­¸ã€ç ”ç©¶ã€è¼”å°èˆ‡æœå‹™å¤–ï¼Œä¸¦èˆ‡é å‚³ã€æ–°ä¸–ç´€è³‡é€šã€é æ±æ–°ä¸–ç´€ã€äºæ±é†«é™¢ç­‰é æ±é›†åœ˜ç”¢å­¸åˆä½œï¼Œæˆæœæ–ç„¶ï¼Œå·²æˆç‚ºæŠ€è·æ•™è‚²æ–°å…¸ç¯„ã€‚

ä¼æ¥­ç°¡ä»‹ï¼šè©³ç´°ä»‹ç´¹ä¼æ¥­çš„æ­·å²ã€å‰µç«‹èƒŒæ™¯ã€å‰µç«‹å¹´åˆ†ã€ä¸»è¦æ¥­å‹™ç­‰ï¼ˆæ­£å¼ã€åš´è¬¹ï¼‰ï¼Œåƒè€ƒä»¥ä¸‹ç¯„ä¾‹æ¶æ§‹ï¼š
äºæ±ç§‘æŠ€å¤§å­¸æ–¼æ°‘åœ‹äº”åä¸ƒå¹´åæœˆï¼Œåœ¨é æ±é›†åœ˜å‰µè¾¦äººå¾æœ‰åº å…ˆç”Ÿçš„ã€Œå¼˜æ–‡æ˜å¾·ï¼Œè‚²æ‰èˆˆåœ‹ã€ç†å¿µä¸‹å‰µè¨­ï¼Œåˆåã€Œç§ç«‹äºæ±å·¥æ¥­æŠ€è—å°ˆç§‘å­¸æ ¡ã€ï¼Œç‚ºå…¨åœ‹ç¬¬ä¸€æ‰€ç§ç«‹äºŒå¹´åˆ¶å°ˆç§‘å­¸æ ¡ï¼Œå…­åäºŒå¹´å…­æœˆå¥‰å‡†æ­£åç‚ºã€Œç§ç«‹äºæ±å·¥æ¥­å°ˆç§‘å­¸æ ¡ã€ï¼Œå…«åä¹å­¸å¹´åº¦ç²æ•™è‚²éƒ¨æ ¸å®šæ”¹åˆ¶ç‚ºã€Œäºæ±æŠ€è¡“å­¸é™¢ã€ï¼Œä¸€ä¸€ï¼¯å­¸å¹´åº¦æ”¹åç‚ºã€Œäºæ±å­¸æ ¡è²¡åœ˜æ³•äººäºæ±ç§‘æŠ€å¤§å­¸ã€ã€‚
æœ¬æ ¡æ•™è·å“¡ç”Ÿäººæ•¸4,397äºº(è³‡æ–™æ™‚é–“2024å¹´)å…¶ä¸­å­¸ç”Ÿ3,931äººï¼Œæ•™è·å“¡å·¥451äººã€‚
"""}
    ]
    
    # **ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ä¸¦å„ªåŒ–åƒæ•¸**
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini",  # ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
        messages=combined_prompt,
        max_tokens=2000,
        temperature=0.3,
        top_p=0.9,  # æ·»åŠ  top_p åƒæ•¸å„ªåŒ–ç”Ÿæˆé€Ÿåº¦
        frequency_penalty=0,  # æ¸›å°‘é‡è¤‡
        presence_penalty=0
    )
    
    generated_text = response.choices[0].message.content.strip()
    
    # **è§£æçµæœ**
    intro_start = generated_text.find("ä¼æ¥­å‰è¨€ï¼š")
    summary_start = generated_text.find("ä¼æ¥­ç°¡ä»‹ï¼š")
    
    if intro_start != -1 and summary_start != -1:
        intro = generated_text[intro_start + len("ä¼æ¥­å‰è¨€ï¼š"):summary_start].strip()
        summary = generated_text[summary_start + len("ä¼æ¥­ç°¡ä»‹ï¼š"):].strip()
    else:
        # å¦‚æœæ ¼å¼è§£æå¤±æ•—ï¼Œå˜—è©¦å…¶ä»–æ–¹å¼
        lines = generated_text.split('\n')
        intro = ""
        summary = ""
        current_section = None
        
        for line in lines:
            if "ä¼æ¥­å‰è¨€" in line:
                current_section = "intro"
                intro += line.replace("ä¼æ¥­å‰è¨€ï¼š", "").strip()
            elif "ä¼æ¥­ç°¡ä»‹" in line:
                current_section = "summary"
                summary += line.replace("ä¼æ¥­ç°¡ä»‹ï¼š", "").strip()
            elif current_section == "intro" and line.strip():
                intro += " " + line.strip()
            elif current_section == "summary" and line.strip():
                summary += " " + line.strip()
    
    return {"intro": intro.strip(), "summary": summary.strip()}

# ğŸ”¹ **é€²ä¸€æ­¥å„ªåŒ–ï¼šä¸¦è¡Œè™•ç†çˆ¬èŸ²å’Œéƒ¨åˆ†æ•¸æ“šæº–å‚™**
async def parallel_scrape_and_prepare(org_name: str, business_id: str, max_results=10):
    """ä¸¦è¡ŒåŸ·è¡Œæœç´¢å’Œçˆ¬å–ï¼Œé€²ä¸€æ­¥æå‡é€Ÿåº¦"""
    # ç²å–æœç´¢çµæœ
    links = await get_google_search_results(org_name, business_id, max_results)
    
    if not links:
        return None
    
    # é™åˆ¶çˆ¬å–æ•¸é‡åˆ°å‰6å€‹æœ€ç›¸é—œçš„éˆæ¥ï¼Œæ¸›å°‘è™•ç†æ™‚é–“
    limited_links = links[:6]
    
    # ä¸¦è¡Œçˆ¬å–å…§å®¹
    results = await batch_extract_web_content(limited_links)
    
    return results

# ğŸ”¹ **ä¿®æ”¹å¾Œçš„ API ç«¯é»**
@word_ai.post("/scrape_company_data")
async def scrape_company_data(request: CompanyRequest):
    """
    ä½¿ç”¨ä¼æ¥­åç¨± & çµ±ç·¨ä¾†æœå°‹ Googleï¼Œæ“·å–ä¼æ¥­è³‡è¨Šï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰
    """
    links = await get_google_search_results(request.org_name, request.business_id, max_results=10)

    if not links:
        return {"message": "æœªæ‰¾åˆ°ç›¸é—œä¼æ¥­ç¶²ç«™"}

    # ä½¿ç”¨ç•°æ­¥æ‰¹é‡çˆ¬å–
    results = await batch_extract_web_content(links)

    return {
        "company": request.org_name,
        "search_results": results
    }

@word_ai.post("/generate_company_info")
async def generate_company_info(request: CompanyRequest):
    """æ¥å—å…¬å¸è³‡æ–™ï¼Œçˆ¬å–æ•¸æ“šä¸¦ç”Ÿæˆä¼æ¥­å‰è¨€èˆ‡ç°¡ä»‹ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""
    
    # ä½¿ç”¨å„ªåŒ–çš„ä¸¦è¡Œçˆ¬å–
    results = await parallel_scrape_and_prepare(request.org_name, request.business_id, max_results=8)
    
    if not results:
        return {
            "company": request.org_name,
            "business_id": request.business_id,
            "intro": "ç„¡æ³•ç²å–è¶³å¤ çš„ä¼æ¥­è³‡è¨Šä¾†ç”Ÿæˆå‰è¨€",
            "summary": "ç„¡æ³•ç²å–è¶³å¤ çš„ä¼æ¥­è³‡è¨Šä¾†ç”Ÿæˆç°¡ä»‹"
        }
    
    # ä½¿ç”¨å„ªåŒ–çš„å–®æ¬¡ AI èª¿ç”¨
    generated_data = await generate_intro_and_summary_from_results_optimized(
        request.org_name, 
        request.business_id, 
        results
    )

    return {
        "company": request.org_name,
        "business_id": request.business_id,
        "intro": generated_data['intro'],
        "summary": generated_data['summary']
    }

# ğŸ”¹ **é¡å¤–å„ªåŒ–ï¼šæ·»åŠ å¿«é€Ÿç‰ˆæœ¬ API**
@word_ai.post("/generate_company_info_fast")
async def generate_company_info_fast(request: CompanyRequest):
    """å¿«é€Ÿç‰ˆæœ¬ï¼šåªè™•ç†å‰3å€‹æœ€ç›¸é—œçš„ç¶²ç«™ï¼Œå¤§å¹…æå‡é€Ÿåº¦"""
    
    # åªç²å–å‰5å€‹æœç´¢çµæœ
    links = await get_google_search_results(request.org_name, request.business_id, max_results=5)
    
    if not links:
        return {
            "company": request.org_name,
            "business_id": request.business_id,
            "intro": "ç„¡æ³•ç²å–è¶³å¤ çš„ä¼æ¥­è³‡è¨Šä¾†ç”Ÿæˆå‰è¨€",
            "summary": "ç„¡æ³•ç²å–è¶³å¤ çš„ä¼æ¥­è³‡è¨Šä¾†ç”Ÿæˆç°¡ä»‹"
        }
    
    # åªçˆ¬å–å‰3å€‹ç¶²ç«™
    limited_links = links[:3]
    results = await batch_extract_web_content(limited_links)
    
    if not results:
        return {
            "company": request.org_name,
            "business_id": request.business_id,
            "intro": "ç„¡æ³•ç²å–è¶³å¤ çš„ä¼æ¥­è³‡è¨Šä¾†ç”Ÿæˆå‰è¨€",
            "summary": "ç„¡æ³•ç²å–è¶³å¤ çš„ä¼æ¥­è³‡è¨Šä¾†ç”Ÿæˆç°¡ä»‹"
        }
    
    # ä½¿ç”¨å„ªåŒ–çš„å–®æ¬¡ AI èª¿ç”¨
    generated_data = await generate_intro_and_summary_from_results_optimized(
        request.org_name, 
        request.business_id, 
        results
    )

    return {
        "company": request.org_name,
        "business_id": request.business_id,
        "intro": generated_data['intro'],
        "summary": generated_data['summary'],
        "note": "å¿«é€Ÿç‰ˆæœ¬ - åŸºæ–¼å‰3å€‹æœ€ç›¸é—œç¶²ç«™ç”Ÿæˆ"
    }