import os
import requests
from openai import OpenAI
from dotenv import load_dotenv  # Import for loading environment variables
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool  # For running synchronous code
from bs4 import BeautifulSoup  # ç”¨æ–¼çˆ¬èŸ²
import re  # æ­£å‰‡è¡¨é”å¼è™•ç†å­—ä¸²
from serpapi import GoogleSearch
import time
import openai

load_dotenv()

word_ai = APIRouter(tags=["wordå°ˆç”¨"])

# å®šç¾©è«‹æ±‚æ¨¡å‹
class CompanyRequest(BaseModel):
    org_name: str
    business_id: str



# ğŸ”¹ **æ­¥é©Ÿ1ï¼šä½¿ç”¨ SERPAPI æœå°‹ä¼æ¥­ç›¸é—œé€£çµ**
def get_google_search_results(org_name: str, business_id: str, max_results=10):
    """
    ä½¿ç”¨ SERPAPI æœå°‹ä¼æ¥­çš„ç›¸é—œé€£çµ
    """
    serpapi_key = os.getenv("SERPAPI_API_KEY")  # ğŸ›‘ æ›¿æ›ç‚ºä½ çš„ SERPAPI API KEY
    search_query = f"{org_name} {business_id}"

    params = {
        "engine": "google",
        "q": search_query,
        "hl": "zh-TW",
        "gl": "tw",
        "api_key": serpapi_key
    }

    search = GoogleSearch(params)
    results = search.get_dict()

    if "organic_results" not in results:
        raise HTTPException(status_code=500, detail="âŒ SERPAPI æœå°‹å¤±æ•—ï¼Œæœªç²å–çµæœ")

    # **æå–å‰ N å€‹ç›¸é—œé€£çµ**
    links = []
    for result in results["organic_results"][:max_results]:  # å–å‰ max_results ç­†
        link = result.get("link")
        if link:
            links.append(link)

    return links


# ğŸ”¹ **æ­¥é©Ÿ2ï¼šæ“·å–ç¶²é å…§å®¹ï¼ˆæ›´å®Œæ•´ï¼‰**
def extract_web_content(url):
    """
    é€²å…¥ç¶²é ï¼Œæ“·å–æ¨™é¡Œã€meta æè¿°ã€é—œéµå­—èˆ‡ä¸»è¦å…§æ–‡
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or "utf-8"

        # è§£æç¶²é å…§å®¹
        soup = BeautifulSoup(response.text, "html.parser")

        # **æ“·å–ç¶²ç«™æ¨™é¡Œ**
        title_tag = soup.find("title")
        title = title_tag.text.strip() if title_tag else "æœªæ‰¾åˆ°æ¨™é¡Œ"

        # **æ“·å– meta description**
        description_tag = soup.find("meta", attrs={"name": "description"})
        description = description_tag["content"].strip() if description_tag else "æœªæ‰¾åˆ°ç¶²ç«™ç°¡ä»‹"

        # **æ“·å– meta keywords**
        keywords_tag = soup.find("meta", attrs={"name": "keywords"})
        keywords = keywords_tag["content"].strip() if keywords_tag else "æœªæ‰¾åˆ°é—œéµå­—"

        # **æ“·å–ä¸»è¦å…§å®¹ï¼ˆæœ€å¤š 10 æ®µï¼‰**
        paragraphs = [p.text.strip() for p in soup.find_all("p") if p.text.strip()]
        headings = [h.text.strip() for h in soup.find_all(["h1", "h2", "h3"]) if h.text.strip()]

        # åˆä½µæ¨™é¡Œèˆ‡å…§æ–‡
        main_content = "\n".join(headings[:5] + paragraphs[:10])  # æœ€å¤š 5 å€‹æ¨™é¡Œ + 10 æ®µæ–‡å­—

        return {
            "url": url,
            "title": title,
            "description": description,
            "keywords": keywords,
            "content": main_content
        }

    except requests.exceptions.RequestException as e:
        print(f"âŒ æ“·å–ç¶²é å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None


# ğŸ”¹ **æ­¥é©Ÿ3ï¼šAPI ç«¯é»**
@word_ai.post("/scrape_company_data")
async def scrape_company_data(request: CompanyRequest):
    """
    ä½¿ç”¨ä¼æ¥­åç¨± & çµ±ç·¨ä¾†æœå°‹ Googleï¼Œæ“·å–ä¼æ¥­è³‡è¨Š
    """
    links = get_google_search_results(request.org_name, request.business_id, max_results=10)

    if not links:
        return {"message": "æœªæ‰¾åˆ°ç›¸é—œä¼æ¥­ç¶²ç«™"}

    results = []
    for link in links:
        data = extract_web_content(link)
        if data:
            results.append(data)
        time.sleep(1)  # é¿å…çˆ¬å–éå¿«è¢«å°é–

    return {
        "company": request.org_name,
        "search_results": results
    }










def generate_intro_and_summary_from_results(company_name: str, company_id: str, results: list):
    """ä½¿ç”¨ OpenAI å…ˆéæ¿¾ä¼æ¥­ç›¸é—œè³‡è¨Šï¼Œå†ç”Ÿæˆä¼æ¥­å‰è¨€èˆ‡ç°¡ä»‹"""
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # **Step 1: å…ˆéæ¿¾èˆ‡ä¼æ¥­ç„¡é—œçš„å…§å®¹**
    filter_prompt = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹ä¼æ¥­è³‡è¨Šå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹ç¶²é å…§å®¹åˆ¤æ–·å“ªäº›èˆ‡å…¬å¸æœ€ç›¸é—œï¼Œåªä¿ç•™é«˜åº¦ç›¸é—œçš„è³‡è¨Šã€‚"},
        {"role": "user", "content": f"é€™æ˜¯ä¼æ¥­ '{company_name}' (ç·¨è™Ÿ: {company_id}) çš„ç›¸é—œè³‡è¨Šã€‚è«‹ç¯©é¸å‡ºèˆ‡è©²ä¼æ¥­ç›´æ¥ç›¸é—œçš„å…§å®¹ï¼Œå¿½ç•¥ç„¡é—œçš„å…§å®¹ï¼Œåªè¿”å›èˆ‡è©²ä¼æ¥­é«˜åº¦ç›¸é—œçš„å…§å®¹ã€‚"}
    ]
    
    for idx, result in enumerate(results):
        filter_prompt.append({
            "role": "user", 
            "content": f"ç¶²é {idx+1}ï¼š\næ¨™é¡Œï¼š{result['title']}\næè¿°ï¼š{result['description']}\nå…§å®¹ï¼š{result['content']}"
        })
    
    filter_response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=filter_prompt,
        max_tokens=2000,
        temperature=0.3
    )
    
    filtered_text = filter_response.choices[0].message.content.strip()
    
    # **Step 2: ç”Ÿæˆä¼æ¥­å‰è¨€èˆ‡ç°¡ä»‹**
    generate_prompt = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¼æ¥­è³‡è¨Šç”ŸæˆåŠ©æ‰‹ï¼Œè«‹æ ¹æ“šæä¾›çš„ä¼æ¥­è³‡è¨Šï¼Œæ•´åˆä¸¦è¼¸å‡ºä»¥ä¸‹å…§å®¹ï¼š"},
        {"role": "user", "content": f"""
        ä¼æ¥­å‰è¨€ï¼šæè¿°ä¼æ¥­çš„é¡˜æ™¯ã€ä½¿å‘½ã€ç¶“ç‡Ÿç†å¿µç­‰ï¼ˆæ­£å¼ã€åš´è¬¹ï¼‰ï¼Œå…§å®¹è«‹åƒè€ƒç¯©é¸å¾Œçš„è³‡è¨Šï¼Œç”Ÿæˆèˆ‡ä»¥ä¸‹å‰è¨€ç¯„ä¾‹å­—æ•¸å·®ä¸å¤šçš„ç­”æ¡ˆ
        å‰è¨€ç¯„ä¾‹(åƒè€ƒå…¶æ¶æ§‹):
        æœ¬æ ¡å‰µæ ¡è¿„ä»Šï¼Œæ­·ä»»æ ¡é•·éµå¾ªå‰µè¾¦äººå‰µæ ¡è·å¿—ï¼Œç¶“ç‡Ÿæ“˜ç•«ï¼Œç©æ¥µç™¼æšã€Œèª ã€å‹¤ã€æ¨¸ã€æ…ã€å‰µæ–°ã€ç²¾ç¥å½¢æˆå„ªè‰¯æ ¡é¢¨ï¼Œä¸¦ç§‰æŒã€Œå‰µæ„ã€å‹™å¯¦ã€å®è§€ã€åˆä½œã€æºé€šã€ç†±å¿±ã€çš„æ•™è‚²ç†å¿µï¼Œä»¥ç§‘æŠ€èˆ‡äººæ–‡èåŒ¯ã€å‰µæ–°èˆ‡å“è³ªä¸¦é‡ã€å°ˆæ¥­èˆ‡é€šè­˜å…¼é¡§ã€ç†è«–èˆ‡å¯¦å‹™çµåˆç‚ºä¸»è»¸ï¼Œç™¼å±•ç‚ºå¯¦å‹™åŒ–ã€è³‡è¨ŠåŒ–ã€äººæ–‡åŒ–ã€å‰µæ–°åŒ–ã€åœ‹éš›åŒ–çš„é«˜ç­‰å­¸åºœã€‚
        ç‚ºæä¾›å­¸ç”Ÿå¤šå…ƒå­¸ç¿’ï¼Œæ•´åˆç›¸é—œå­¸è¡“è³‡æºï¼Œæœ¬æ ¡ç‰¹æˆç«‹é›»é€šã€å·¥ç¨‹ã€é†«è­·æš¨ç®¡ç†ä¸‰å¤§å­¸é™¢ï¼Œè—‰ç”±å„å­¸ç³»çš„åˆä½œã€å› æ‡‰ç”¢æ¥­éœ€æ±‚ï¼Œé–‹è¨­ç›¸é—œå­¸ç¨‹ï¼Œè®“å­¸ç”Ÿé€éè·¨é ˜åŸŸå­¸ç¿’ï¼Œæå‡å°ˆæ¥­çŸ¥èƒ½èˆ‡è·å ´ç«¶çˆ­åŠ›ã€‚
        æœ¬æ ¡ç©æ¥µæå‡æ•™å­¸ã€ç ”ç©¶ã€è¼”å°èˆ‡æœå‹™å¤–ï¼Œä¸¦èˆ‡é å‚³ã€æ–°ä¸–ç´€è³‡é€šã€é æ±æ–°ä¸–ç´€ã€äºæ±é†«é™¢ç­‰é æ±é›†åœ˜ç”¢å­¸åˆä½œï¼Œæˆæœæ–ç„¶ï¼Œå·²æˆç‚ºæŠ€è·æ•™è‚²æ–°å…¸ç¯„ã€‚
        
        ä¼æ¥­ç°¡ä»‹ï¼šè©³ç´°ä»‹ç´¹ä¼æ¥­çš„æ­·å²ã€å‰µç«‹èƒŒæ™¯ã€å‰µç«‹å¹´åˆ†ã€ä¸»è¦æ¥­å‹™ç­‰ï¼ˆæ­£å¼ã€åš´è¬¹ï¼‰ï¼Œå…§å®¹è«‹åƒè€ƒç¯©é¸å¾Œçš„è³‡è¨Šï¼Œç”Ÿæˆèˆ‡ä»¥ä¸‹ç°¡ä»‹ç¯„ä¾‹å­—æ•¸å·®ä¸å¤šçš„ç­”æ¡ˆ
        ç°¡ä»‹ç¯„ä¾‹(åƒè€ƒå…¶æ¶æ§‹):
        äºæ±ç§‘æŠ€å¤§å­¸æ–¼æ°‘åœ‹äº”åä¸ƒå¹´åæœˆï¼Œåœ¨é æ±é›†åœ˜å‰µè¾¦äººå¾æœ‰åº å…ˆç”Ÿçš„ã€Œå¼˜æ–‡æ˜å¾·ï¼Œè‚²æ‰èˆˆåœ‹ã€ç†å¿µä¸‹å‰µè¨­ï¼Œåˆåã€Œç§ç«‹äºæ±å·¥æ¥­æŠ€è—å°ˆç§‘å­¸æ ¡ã€ï¼Œç‚ºå…¨åœ‹ç¬¬ä¸€æ‰€ç§ç«‹äºŒå¹´åˆ¶å°ˆç§‘å­¸æ ¡ï¼Œå…­åäºŒå¹´å…­æœˆå¥‰å‡†æ­£åç‚ºã€Œç§ç«‹äºæ±å·¥æ¥­å°ˆç§‘å­¸æ ¡ã€ï¼Œå…«åä¹å­¸å¹´åº¦ç²æ•™è‚²éƒ¨æ ¸å®šæ”¹åˆ¶ç‚ºã€Œäºæ±æŠ€è¡“å­¸é™¢ã€ï¼Œä¸€ä¸€ï¼¯å­¸å¹´åº¦æ”¹åç‚ºã€Œäºæ±å­¸æ ¡è²¡åœ˜æ³•äººäºæ±ç§‘æŠ€å¤§å­¸ã€ã€‚
        æœ¬æ ¡æ•™è·å“¡ç”Ÿäººæ•¸4,397äºº(è³‡æ–™æ™‚é–“2024å¹´)å…¶ä¸­å­¸ç”Ÿ3,931äººï¼Œæ•™è·å“¡å·¥451äººã€‚

        é€™æ˜¯ä¼æ¥­ '{company_name}' çš„ç¯©é¸å¾Œè³‡è¨Šï¼Œè«‹æ ¹æ“šé€™äº›å…§å®¹æ’°å¯«ä¼æ¥­å‰è¨€èˆ‡ä¼æ¥­ç°¡ä»‹ã€‚\n\n{filtered_text}
        """}
    ]
    
    generate_response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=generate_prompt,
        max_tokens=1500,
        temperature=0.5
    )
    
    generated_text = generate_response.choices[0].message.content.strip()
    
    # **Step 3: æ“·å–ä¼æ¥­å‰è¨€èˆ‡ç°¡ä»‹**
    intro_start = generated_text.find("ä¼æ¥­å‰è¨€ï¼š")
    summary_start = generated_text.find("ä¼æ¥­ç°¡ä»‹ï¼š")
    
    intro = generated_text[intro_start + len("ä¼æ¥­å‰è¨€ï¼š"):summary_start].strip() if intro_start != -1 else ""
    summary = generated_text[summary_start + len("ä¼æ¥­ç°¡ä»‹ï¼š"):].strip() if summary_start != -1 else ""
    
    return {"intro": intro, "summary": summary}


# ä¿®æ”¹ API ç«¯é»ï¼Œæ•´åˆçˆ¬èŸ²èˆ‡ OpenAI çš„åŠŸèƒ½
@word_ai.post("/generate_company_info")
async def generate_company_info(request: CompanyRequest):
    """æ¥å—å…¬å¸è³‡æ–™ï¼Œçˆ¬å–æ•¸æ“šä¸¦ç”Ÿæˆä¼æ¥­å‰è¨€èˆ‡ç°¡ä»‹"""
    # ç²å–çˆ¬èŸ²çµæœ
    links = get_google_search_results(request.org_name, request.business_id, max_results=10)
    results = []

    for link in links:
        data = extract_web_content(link)
        if data:
            results.append(data)
    
    # ç”Ÿæˆå‰è¨€èˆ‡ç°¡ä»‹
    generated_data = generate_intro_and_summary_from_results(request.org_name, request.business_id, results)

    return {
        "company": request.org_name,
        "business_id": request.business_id,
        "intro": generated_data['intro'],
        "summary": generated_data['summary']
    }

