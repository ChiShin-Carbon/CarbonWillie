import requests
import time
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from bs4 import BeautifulSoup
from serpapi import GoogleSearch
import os
from fastapi import APIRouter

word_bug = APIRouter(tags=["wordçˆ¬èŸ²å°ˆç”¨"])

# ğŸ”¹ **ä¼æ¥­è«‹æ±‚æ¨¡å‹**
class CompanyRequest(BaseModel):
    org_name: str
    business_id: str  # çµ±ä¸€ç·¨è™Ÿ


# ğŸ”¹ **æ­¥é©Ÿ1ï¼šä½¿ç”¨ SERPAPI æœå°‹ä¼æ¥­ç›¸é—œé€£çµ**
def get_google_search_results(org_name: str, business_id: str, max_results=10):
    """
    ä½¿ç”¨ SERPAPI æœå°‹ä¼æ¥­çš„ç›¸é—œé€£çµ
    """
    serpapi_key = os.getenv("SERPAPI_API_KEY")  # ğŸ›‘ æ›¿æ›ç‚ºä½ çš„ SERPAPI API KEY
    search_query = f"{org_name} {business_id}"

    params = {
        "engine": "google",
        "q": search_query,
        "hl": "zh-TW",
        "gl": "tw",
        "api_key": serpapi_key
    }

    search = GoogleSearch(params)
    results = search.get_dict()

    if "organic_results" not in results:
        raise HTTPException(status_code=500, detail="âŒ SERPAPI æœå°‹å¤±æ•—ï¼Œæœªç²å–çµæœ")

    # **æå–å‰ N å€‹ç›¸é—œé€£çµ**
    links = []
    for result in results["organic_results"][:max_results]:  # å–å‰ max_results ç­†
        link = result.get("link")
        if link:
            links.append(link)

    return links


# ğŸ”¹ **æ­¥é©Ÿ2ï¼šæ“·å–ç¶²é å…§å®¹ï¼ˆæ›´å®Œæ•´ï¼‰**
def extract_web_content(url):
    """
    é€²å…¥ç¶²é ï¼Œæ“·å–æ¨™é¡Œã€meta æè¿°ã€é—œéµå­—èˆ‡ä¸»è¦å…§æ–‡
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or "utf-8"

        # è§£æç¶²é å…§å®¹
        soup = BeautifulSoup(response.text, "html.parser")

        # **æ“·å–ç¶²ç«™æ¨™é¡Œ**
        title_tag = soup.find("title")
        title = title_tag.text.strip() if title_tag else "æœªæ‰¾åˆ°æ¨™é¡Œ"

        # **æ“·å– meta description**
        description_tag = soup.find("meta", attrs={"name": "description"})
        description = description_tag["content"].strip() if description_tag else "æœªæ‰¾åˆ°ç¶²ç«™ç°¡ä»‹"

        # **æ“·å– meta keywords**
        keywords_tag = soup.find("meta", attrs={"name": "keywords"})
        keywords = keywords_tag["content"].strip() if keywords_tag else "æœªæ‰¾åˆ°é—œéµå­—"

        # **æ“·å–ä¸»è¦å…§å®¹ï¼ˆæœ€å¤š 10 æ®µï¼‰**
        paragraphs = [p.text.strip() for p in soup.find_all("p") if p.text.strip()]
        headings = [h.text.strip() for h in soup.find_all(["h1", "h2", "h3"]) if h.text.strip()]

        # åˆä½µæ¨™é¡Œèˆ‡å…§æ–‡
        main_content = "\n".join(headings[:5] + paragraphs[:10])  # æœ€å¤š 5 å€‹æ¨™é¡Œ + 10 æ®µæ–‡å­—

        return {
            "url": url,
            "title": title,
            "description": description,
            "keywords": keywords,
            "content": main_content
        }

    except requests.exceptions.RequestException as e:
        print(f"âŒ æ“·å–ç¶²é å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None


# ğŸ”¹ **æ­¥é©Ÿ3ï¼šAPI ç«¯é»**
@word_bug.post("/scrape_company_data")
async def scrape_company_data(request: CompanyRequest):
    """
    ä½¿ç”¨ä¼æ¥­åç¨± & çµ±ç·¨ä¾†æœå°‹ Googleï¼Œæ“·å–ä¼æ¥­è³‡è¨Š
    """
    links = get_google_search_results(request.org_name, request.business_id, max_results=10)

    if not links:
        return {"message": "æœªæ‰¾åˆ°ç›¸é—œä¼æ¥­ç¶²ç«™"}

    results = []
    for link in links:
        data = extract_web_content(link)
        if data:
            results.append(data)
        time.sleep(1)  # é¿å…çˆ¬å–éå¿«è¢«å°é–

    return {
        "company": request.org_name,
        "search_results": results
    }
