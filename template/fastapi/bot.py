from openai import OpenAI  # Import the OpenAI class
import os
from dotenv import load_dotenv  # Import for loading environment variables
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool  # For running synchronous code
from connect.connect import connectDB  # Import the custom connect function
from PyPDF2 import PdfReader
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document  # Import Document class
import json

from collections import Counter

# Load environment variables
load_dotenv()

# Initialize FastAPI router
botapi = APIRouter()

# Define a request model for the incoming data
class MessageRequest(BaseModel):
    message: str

@botapi.post("/botapi")
async def botmessage(request: MessageRequest):

    # Set the OpenAI API key
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))  # Pass the API key directly here
    if not client:
        raise HTTPException(status_code=500, detail="OpenAI API key is not set in the environment.")
    
    # If intent is query, proceed to generate the query
    tables_path = "./CreateTables.txt"
    with open(tables_path, 'r', encoding='utf-8') as file:
        tables_content = file.read()

    user_message = request.message

    intent_response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": f"""
You are an intelligent assistant that replies to the user's questions with precision. 
Your primary focus is to understand the user's intent before responding. 

You have the following tools:
- query: Use this when the user asks for information that requires external sources or research, you have these table: {tables_content}
- answer: Use this when the user asks about Carbon Footprint Verification, including its process, standards, calculations, or related details.
- others: Use this for any other questions or requests unrelated to Carbon Footprint Verification or external queries.

Your task is to:
1. **Accurately determine the user's intent** and respond with one of the following intents ONLY:
    - query
    - answer
    - others
            """},
            {"role": "user", "content": user_message},
        ]
    )
    
    # Extract intent from the response
    intent = intent_response.choices[0].message.content.strip().lower()
    print(f"Intent: {intent}")

    # Check the determined intent
    if intent == "query":
        # If intent is query, proceed to generate the query
        tables_path = "./CreateTables.txt"
        with open(tables_path, 'r', encoding='utf-8') as file:
            tables_content = file.read()
        
        query_intent = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"""
                æˆ‘å€‘çš„è³‡æ–™åº«æœ‰ä»¥ä¸‹tableï¼Œè«‹åˆ¤æ–·ä½¿ç”¨è€…çš„å•é¡Œå±¬æ–¼å“ªå€‹tableï¼Œä¸¦çµ¦å‡ºqueryçš„æŒ‡ä»¤
                (æ³¨æ„ï¼š
                1.åªè¦çµ¦å‡ºSQLæŒ‡ä»¤å³å¯ï¼Œä¸éœ€markdownæ ¼å¼
                2.æ¯æ¬¡éƒ½select * from table_nameå³å¯)
                {tables_content}
                """},
                {"role": "user", "content": user_message},
            ]
        )

        conn = connectDB()  # Establish connection using your custom connect function
        if conn:
            cursor = conn.cursor()
            # Secure SQL query using a parameterized query to prevent SQL injection
            cursor.execute(query_intent.choices[0].message.content)
            # Fetch all records
            records = cursor.fetchall()
            conn.close()

            result = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": f"""
                    ä¾æ“š{user_message}åœ¨{records}ä¸­å°‹æ‰¾ä½¿ç”¨è€…è¦çš„è³‡æ–™ä¸¦å›è¦†ï¼Œå¦‚æœæ²’ç‰¹åˆ¥è¦æ±‚å°±å›è¦†æ‰€æœ‰è³‡æ–™
                    """},
                ]
            )
            
            result_message = result.choices[0].message.content
            print(result_message)
            return {"response": result_message}
        else:
            print("Could not connect to the database.")
            return {"response": "Could not connect to the database."}

    elif intent == "answer":

        def extract_text_from_pdf(file_path):
            """Extracts text from a PDF file."""
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text
            return text

        persist_directory = "./RAGè³‡è¨Š/vectorstore_db"
        populated_flag = os.path.join(persist_directory, ".populated")

        folder_path = './RAGè³‡è¨Š'
        all_pdfname = []

        # Ensure the directory for ChromaDB exists
        os.makedirs(persist_directory, exist_ok=True)

        # 1. Initialize Vector Store
        vectorstore = Chroma(
            collection_name="full_documents", 
            embedding_function=OpenAIEmbeddings(),
            persist_directory=persist_directory
        )

        # 2. æº–å‚™æ‰€æœ‰ PDF æª”å
        for filename in os.listdir(folder_path):
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(folder_path, filename)
                all_pdfname.append(pdf_path)

        # 3. å¦‚æœå‘é‡è³‡æ–™åº«é‚„æ²’å»ºç½®ï¼Œå‰‡å°‡ PDF åŠ å…¥å‘é‡è³‡æ–™åº«
        if not os.path.exists(populated_flag):
            for pdf_name in all_pdfname:
                file_path = os.path.join(pdf_name)
                pdf_text = extract_text_from_pdf(file_path)

                if not pdf_text.strip():
                    print(f"âš ï¸ PDF '{pdf_name}' æ²’æœ‰æå–åˆ°ä»»ä½•æ–‡å­—ã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                    continue

                child_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
                text_chunks = child_splitter.split_text(pdf_text)

                if not text_chunks:
                    print(f"âš ï¸ PDF '{pdf_name}' çš„æ–‡å­—åˆ‡å‰²çµæœç‚ºç©ºã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                    continue

                documents = [Document(page_content=chunk) for chunk in text_chunks if chunk.strip()]

                if not documents:
                    print(f"âš ï¸ PDF '{pdf_name}' çš„ documents åˆ—è¡¨ç‚ºç©ºã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                    continue

                # åŠ å…¥å‘é‡è³‡æ–™åº«
                vectorstore.add_documents(documents)

            with open(populated_flag, "w") as f:
                f.write("populated")
            print("âœ… æ‰€æœ‰ PDF çš„å…§å®¹å·²åŠ å…¥å‘é‡è³‡æ–™åº«ã€‚")
        else:
            print("ğŸ”„ å‘é‡è³‡æ–™åº«å·²æœ‰è³‡æ–™")

        # ---- å‡½æ•¸å°è£ï¼šæª¢ç´¢ top-k chunks ----
        def retrieve_top_k_chunks(user_query, k=5):
            """
            å¾ vectorstore ä¸­æª¢ç´¢æœ€ç›¸é—œçš„ top-k chunksã€‚
            """
            sub_docs = vectorstore.similarity_search(user_query, k=k)
            return [doc.page_content for doc in sub_docs]

        # ---- å‡½æ•¸å°è£ï¼šPairwise æ¯”è¼ƒ ----
        def pairwise_compare(query, chunk1, chunk2):
            """
            ä½¿ç”¨ GPT å°å…©å€‹ chunk é€²è¡Œ Pairwise æ¯”è¼ƒï¼Œåˆ¤æ–·å“ªå€‹æ›´ç›¸é—œã€‚
            å›å‚³æ›´ç›¸é—œçš„ chunkã€‚
            """
            compare_prompt = f"""
    ä½ æ˜¯ä¸€å€‹å¯©æ ¸å“¡ï¼Œè² è²¬æ¯”è¼ƒä»¥ä¸‹å…©å€‹å…§å®¹ç‰‡æ®µï¼Œåˆ¤æ–·å“ªå€‹æ›´é©åˆå›ç­”ä½¿ç”¨è€…å•é¡Œã€‚
    è«‹ä½ è¼¸å‡ºä»¥ä¸‹JSONæ ¼å¼ï¼Œç„¡å…¶ä»–æ–‡å­—ï¼š
    {{
      "more_relevant_chunk": "æ›´ç›¸é—œçš„å…§å®¹ç‰‡æ®µ"
    }}

    ä½¿ç”¨è€…å•é¡Œ: {query}

    å…§å®¹ç‰‡æ®µ 1:
    {chunk1}

    å…§å®¹ç‰‡æ®µ 2:
    {chunk2}
            """

            # num_retries = 5  # è¨­ç½®å¤šæ¬¡è©¢å•æ¬¡æ•¸
            # responses = []

            # for _ in range(num_retries):
            #     compare_response = client.chat.completions.create(
            #         model="gpt-4o",
            #         temperature=0,  # ç¢ºä¿çµæœç¢ºå®šæ€§
            #         messages=[
            #             {"role": "system", "content": (
            #                 "ä½ æ˜¯ä¸€ä½ç²¾é€šæ–‡æœ¬æ¯”è¼ƒçš„å°ˆå®¶ï¼Œè«‹ä½¿ç”¨é‚è¼¯æ¨ç†åˆ†æä»¥ä¸‹å…§å®¹ã€‚\n"
            #                 "è«‹éµå¾ªä»¥ä¸‹åŸå‰‡ï¼š\n"
            #                 "1. **æ€ç¶­éˆæ‹†è§£ (CoT)**ï¼šé€æ­¥åˆ†è§£æ¯”è¼ƒæ¨™æº–ï¼Œç¢ºä¿åš´è¬¹æ¨ç†ã€‚\n"
            #                 "2. **é©—ç®—èˆ‡é©—è­‰ (Analyze Rate)**ï¼šæª¢æŸ¥æ¯”è¼ƒçµæœæ˜¯å¦åˆç†ï¼Œä¸¦æä¾›ç°¡å–®é©—è­‰ã€‚\n"
            #                 "3. **è‡ªæ´½æ€§ (Self-Consistency)**ï¼šè‹¥å­˜åœ¨å¤šç¨®å¯èƒ½çµæœï¼Œè«‹çµ±è¨ˆæœ€å¸¸è¦‹çš„çµè«–ã€‚\n"
            #                 "4. **æ€ç¶­åœ–è­œ (Graphs of Thought, GoT)**ï¼šä½¿ç”¨çµæ§‹åŒ–æ–¹å¼é€²è¡Œæ¯”è¼ƒã€‚\n"
            #                 "5. **è«‹æ ¹æ“šç¯„ä¾‹è¼¸å‡º JSON æ ¼å¼ï¼Œç¢ºä¿å…§å®¹çµæ§‹åŒ–**ã€‚"
            #             )},
            #             {"role": "user", "content": compare_prompt}
            #         ]
            #     )

            #     content = compare_response.choices[0].message.content.strip()
                
            #     try:
            #         result = json.loads(content)
            #         responses.append(result.get("more_relevant_chunk", ""))
            #     except Exception:
            #         responses.append("")  # è§£æå¤±æ•—æ™‚ï¼Œè¨˜éŒ„ç©ºå­—ä¸²ä»¥é¿å…å ±éŒ¯

            # # **ä½¿ç”¨ Self-Consistency ç¢ºä¿ç©©å®šè¼¸å‡º**
            # most_common_response = Counter(responses).most_common(1)[0][0]

            # # **ç¢ºä¿çµæœä¸ç‚ºç©ºï¼Œå¦å‰‡å›å‚³ chunk1**
            # return most_common_response if most_common_response else chunk1


            compare_response = client.chat.completions.create(
                model="gpt-4o",
                temperature=0,  # ç¢ºä¿çµæœç¢ºå®šæ€§
                messages=[
                    {"role": "system", "content": "è«‹å°ˆæ³¨æ–¼æ¯”è¼ƒï¼Œä¸è¦è¼¸å‡ºå¤šé¤˜è§£é‡‹ã€‚"},
                    {"role": "user", "content": compare_prompt}
                ]
            )

            content = compare_response.choices[0].message.content.strip()

            try:
                result = json.loads(content)
                return result.get("more_relevant_chunk", chunk1)  # é è¨­å›å‚³ chunk1
            except Exception:
                return chunk1  # è‹¥è§£æå¤±æ•—ï¼Œå›å‚³ chunk1

        # ---- å‡½æ•¸å°è£ï¼šæ·˜æ±°æ©Ÿåˆ¶ ----
        def eliminate_chunks(query, chunks):
            """
            å° chunks é€²è¡Œ Pairwise æ¯”è¼ƒï¼Œæ·˜æ±°æ‰è¼ƒä¸ç›¸é—œçš„ chunkã€‚
            æœ€çµ‚ç•™ä¸‹ 2 å€‹æœ€ç›¸é—œçš„ chunkã€‚
            """
            while len(chunks) > 1:
                # æ¯æ¬¡æ¯”è¼ƒå‰å…©å€‹ chunkï¼Œæ·˜æ±°è¼ƒä¸ç›¸é—œçš„
                more_relevant = pairwise_compare(query, chunks[0], chunks[1])
                if more_relevant == chunks[0]:
                    chunks.pop(1)  # æ·˜æ±° chunks[1]
                    print("eliminate 1 chunk")
                else:
                    chunks.pop(0)  # æ·˜æ±° chunks[0]
                    print("eliminate 1 chunk")

            return chunks

        # ---- å‡½æ•¸å°è£ï¼šç¸½çµ ----
        def summarize_chunks(query, chunks):
            """
            å°ç•™ä¸‹çš„ chunks é€²è¡Œç¸½çµï¼Œç”Ÿæˆæœ€çµ‚å›ç­”ã€‚
            """
            summarize_response = client.chat.completions.create(
                model="gpt-4o",
                temperature=0,  # è®“æ¨¡å‹å›è¦†ç›¡å¯èƒ½ä¸€è‡´
                messages=[
                    {
                        "role": "system",
                        "content": f"ç”¨#zh-twå›ç­”ï¼Œæ ¹æ“šæ‚¨çš„å•é¡Œ({query})ï¼Œä»¥ä¸‹æ˜¯å¾ç›¸é—œå…§å®¹ä¸­æå–çš„ç¸½çµï¼š"
                    },
                    {
                        "role": "user",
                        "content": "\n\n".join(chunks)
                    },
                ]
            )
            return summarize_response.choices[0].message.content

        # ---- main process ----
        query = user_message

        # 1. æª¢ç´¢ top-5 chunks
        top_chunks = retrieve_top_k_chunks(query, k=5)
        print(f"Step1:retrieve top 5 chunks, numbers of chunks:{len(top_chunks)}")

        # 2. Pairwise æ¯”è¼ƒï¼Œæ·˜æ±°æ‰ 4 å€‹ chunkï¼Œç•™ä¸‹ 1 å€‹
        final_chunks = eliminate_chunks(query, top_chunks)
        print(f"final chunks number:{len(final_chunks)}")

        # 3. å°ç•™ä¸‹çš„ chunk é€²è¡Œç¸½çµ
        final_answer = summarize_chunks(query, final_chunks)

        return {"response": final_answer}
    else:
        # Handle other intents here
        print("Intent is others.")
        return {"response": "æŠ±æ­‰ï¼Œç¢³æ™ºéƒåƒ…èƒ½å›ç­”è³‡æ–™åº«ä¸­å’Œç¢³ç›¤æŸ¥ç›¸é—œçš„å•é¡Œ"}


    

    # try:
    #     # Run the OpenAI API call in a thread pool
    #     completion = await run_in_threadpool(
    #         openai.ChatCompletion.create,
    #         model="gpt-4",  # Use a valid model
    #         messages=[
    #             {"role": "system", "content": "You are a helpful assistant."},
    #             {"role": "user", "content": user_message},
    #         ]
    #     )

    #     # Return the response content as JSON
    #     return {"response": completion.choices[0].message.content}

    # except Exception as e:
    #     # Log the error and raise HTTPException
    #     print(f"Error occurred: {str(e)}")  # Log error message
    #     raise HTTPException(status_code=500, detail=f"An error occurred while processing the request: {str(e)}")
