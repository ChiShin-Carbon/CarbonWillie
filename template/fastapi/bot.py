from openai import OpenAI
import os
from dotenv import load_dotenv
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool
from connect.connect import connectDB
from PyPDF2 import PdfReader
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
import re
from collections import Counter
from datetime import datetime
from decimal import Decimal
from typing import Dict, List, Any, Optional, Tuple
import functools
import time

# Load environment variables
load_dotenv()

# Initialize FastAPI router
botapi = APIRouter(tags=["bot"])

# Define a request model for the incoming data
class MessageRequest(BaseModel):
    message: str

# Cache for frequently used content
_tables_content_cache = None
_vectorstore_cache = None

# ========================================
# Optimized SQL Query Generation Class
# ========================================

class SQLQueryEnhancer:
    def __init__(self, client: OpenAI):
        self.client = client
        self.common_errors = {
            "no such table": "æ‰¾ä¸åˆ°æŒ‡å®šçš„è³‡æ–™è¡¨",
            "syntax error": "SQLèªæ³•éŒ¯èª¤",
            "no such column": "æ‰¾ä¸åˆ°æŒ‡å®šçš„æ¬„ä½",
            "ambiguous column": "æ¬„ä½åç¨±æ¨¡ç³Šï¼Œéœ€è¦æŒ‡å®šè¡¨æ ¼åç¨±"
        }
    
    @functools.lru_cache(maxsize=1)
    def get_enhanced_tables_content(self) -> str:
        """Read and return enhanced database tables information (cached)."""
        global _tables_content_cache
        if _tables_content_cache:
            return _tables_content_cache
            
        tables_path = "./CreateTables.txt"
        try:
            with open(tables_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            # Simplified enhanced content (reduced size)
            enhanced_content = content + """
            
# å¸¸ç”¨æŸ¥è©¢æ¨¡å¼ï¼š
# æ™‚é–“: WHERE Doc_date BETWEEN '2024-01-01' AND '2024-12-31'
# èšåˆ: SELECT SUM(liters) FROM Vehicle WHERE YEAR(Doc_date) = 2024
# éƒ¨é–€ï¼š1=ç®¡ç†,2=è³‡è¨Š,3=æ¥­å‹™,4=é–€è¨º,5=å¥æª¢,6=æª¢é©—,7=å…¶ä»–
# è·ä½ï¼š1=ç¸½ç¶“ç†,2=å‰¯ç¸½,3=ä¸»ç®¡,4=å‰¯ä¸»ç®¡,5=çµ„é•·,6=å…¶ä»–
# è§’è‰²ï¼š0=ç³»çµ±ç®¡ç†å“¡,1=é¡§å•,2=ä¼æ¥­ç”¨æˆ¶
            """
            _tables_content_cache = enhanced_content
            return enhanced_content
        except Exception as e:
            print(f"Error reading tables content: {e}")
            return ""

    def generate_sql_with_context(self, user_message: str) -> str:
        """Generate SQL query with reduced context for faster processing."""
        tables_content = self.get_enhanced_tables_content()
        
        # Simplified system prompt
        system_prompt = f"""
ä½ æ˜¯SQLæŸ¥è©¢ç”Ÿæˆå°ˆå®¶ã€‚åªå›å‚³å¯åŸ·è¡Œçš„SQLæŸ¥è©¢èªå¥ï¼Œä¸è¦ä»»ä½•è§£é‡‹ã€‚

è¦å‰‡ï¼š
1. åªå›å‚³SQLï¼Œç„¡markdownæ ¼å¼
2. ä½¿ç”¨è‹±æ–‡æ¨™é»ç¬¦è™Ÿ
3. æ—¥æœŸæ ¼å¼ï¼š'YYYY-MM-DD'
4. JOINé—œè¯ï¼šusers.user_id â†” å…¶ä»–è¡¨user_id

è³‡æ–™åº«çµæ§‹ï¼š{tables_content}

å•é¡Œï¼š{user_message}
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                temperature=0,  # Zero temperature for consistency and speed
                max_tokens=200,  # Limit tokens for faster response
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ]
            )
            
            sql_query = response.choices[0].message.content.strip()
            sql_query = self.sanitize_sql_query(sql_query)
            return sql_query
            
        except Exception as e:
            print(f"Error generating SQL: {e}")
            raise Exception("SQLç”Ÿæˆå¤±æ•—")

    def sanitize_sql_query(self, sql_query: str) -> str:
        """Clean SQL query (optimized)."""
        # Remove markdown
        sql_query = re.sub(r'```(?:sql)?\n?|```', '', sql_query)
        
        # Replace Chinese punctuation
        replacements = {'ï¼Œ': ',', 'ï¼›': ';', 'ï¼š': ':', '"': '"', '"': '"'}
        for chinese, english in replacements.items():
            sql_query = sql_query.replace(chinese, english)
        
        sql_query = ' '.join(sql_query.split())
        
        # Quick security check
        dangerous = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE']
        if any(keyword in sql_query.upper() for keyword in dangerous):
            raise Exception("ä¸å…è¨±åŸ·è¡Œå±éšªæ“ä½œ")
        
        return sql_query

    def execute_query_with_metadata(self, sql_query: str) -> Tuple[List[Dict], Dict]:
        """Execute query (optimized)."""
        conn = connectDB()
        if not conn:
            raise Exception("ç„¡æ³•é€£æ¥è³‡æ–™åº«")
        
        try:
            cursor = conn.cursor()
            start_time = time.time()
            cursor.execute(sql_query)
            
            # Get results
            raw_records = cursor.fetchall()
            column_info = [{"name": desc[0]} for desc in cursor.description] if cursor.description else []
            
            # Simplified parsing
            parsed_records = []
            for record in raw_records:
                parsed_record = {}
                for i, value in enumerate(record):
                    col_name = column_info[i]["name"] if i < len(column_info) else f"col_{i}"
                    
                    if value is None:
                        parsed_record[col_name] = None
                    elif isinstance(value, Decimal):
                        parsed_record[col_name] = float(value)
                    elif isinstance(value, datetime):
                        parsed_record[col_name] = value.strftime('%Y-%m-%d %H:%M:%S')
                    elif hasattr(value, 'date'):
                        parsed_record[col_name] = value.strftime('%Y-%m-%d')
                    else:
                        parsed_record[col_name] = str(value)
                
                parsed_records.append(parsed_record)
            
            metadata = {
                "total_records": len(raw_records),
                "execution_time": round(time.time() - start_time, 2)
            }
            
            return parsed_records, metadata
            
        except Exception as e:
            raise Exception(f"æŸ¥è©¢éŒ¯èª¤: {str(e)}")
        finally:
            conn.close()

    def generate_user_friendly_response(self, user_message: str, parsed_records: List[Dict], 
                                      metadata: Dict) -> str:
        """Generate response (optimized)."""
        
        # Quick fallback for simple cases
        if not parsed_records:
            return "æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è³‡æ–™ã€‚"
        
        total_records = len(parsed_records)
        if total_records == 1:
            # Single record - simple format
            record = parsed_records[0]
            result = "æŸ¥è©¢çµæœï¼š\n"
            for key, value in record.items():
                result += f"{key}: {value}\n"
            return result
        
        # Simplified response generation for multiple records
        response_prompt = f"""
æ ¹æ“šç”¨æˆ¶å•é¡Œç”Ÿæˆç°¡æ½”å›ç­”ï¼š

å•é¡Œï¼š{user_message}
è³‡æ–™ç­†æ•¸ï¼š{total_records}
çµæœï¼š{json.dumps(parsed_records[:3], ensure_ascii=False)}

è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”å•é¡Œ
2. ç°¡æ½”æ˜ç­
3. å¦‚æœè³‡æ–™å¤šï¼Œæä¾›æ‘˜è¦
4. è½‰æ›ä»£ç¢¼ï¼šéƒ¨é–€(1=ç®¡ç†,2=è³‡è¨Š,3=æ¥­å‹™,4=é–€è¨º,5=å¥æª¢,6=æª¢é©—,7=å…¶ä»–)
        """
        
        try:
            result = self.client.chat.completions.create(
                model="gpt-4o-mini",
                temperature=0,
                max_tokens=300,  # Limit for faster response
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æ•¸æ“šåˆ†æåŠ©æ‰‹ï¼Œæä¾›ç°¡æ½”æº–ç¢ºçš„å›ç­”ã€‚"},
                    {"role": "user", "content": response_prompt}
                ]
            )
            
            return result.choices[0].message.content
            
        except Exception as e:
            print(f"Error generating response: {e}")
            return f"æ‰¾åˆ° {total_records} ç­†è³‡æ–™ã€‚"


# ========================================
# Optimized Main Bot Functions
# ========================================

@botapi.post("/botapi")
async def botmessage(request: MessageRequest):
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    if not client:
        raise HTTPException(status_code=500, detail="OpenAI API key not set.")
    
    user_message = request.message

    # Simplified intent detection
    intent_prompt = f"""
åˆ¤æ–·ç”¨æˆ¶æ„åœ–ï¼Œåªå›ç­”ä»¥ä¸‹ä¹‹ä¸€ï¼š
- query: éœ€è¦æŸ¥è©¢è³‡æ–™åº«è³‡æ–™
- answer: è©¢å•ç¢³ç›¤æŸ¥ç›¸é—œçŸ¥è­˜
- others: å…¶ä»–å•é¡Œ

å•é¡Œï¼š{user_message}
    """
    
    intent_response = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        max_tokens=10,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯æ„åœ–è­˜åˆ¥å°ˆå®¶ï¼Œåªå›ç­”intenté¡å‹ã€‚"},
            {"role": "user", "content": intent_prompt}
        ]
    )
    
    intent = intent_response.choices[0].message.content.strip().lower()
    print(f"Intent: {intent}")

    if intent == "query":
        return await handle_query_intent(client, user_message)
    elif intent == "answer":
        return await handle_answer_intent_optimized(client, user_message)
    else:
        return {"response": "æŠ±æ­‰ï¼Œç¢³æ™ºéƒåƒ…èƒ½å›ç­”è³‡æ–™åº«ä¸­å’Œç¢³ç›¤æŸ¥ç›¸é—œçš„å•é¡Œ"}


async def handle_query_intent(client, user_message):
    """Optimized query handler."""
    enhancer = SQLQueryEnhancer(client)
    
    try:
        sql_query = enhancer.generate_sql_with_context(user_message)
        print(f"Generated SQL: {sql_query}")
        
        parsed_records, metadata = enhancer.execute_query_with_metadata(sql_query)
        response = enhancer.generate_user_friendly_response(user_message, parsed_records, metadata)
        
        return {"response": response}
        
    except Exception as e:
        error_msg = str(e)
        print(f"Query error: {error_msg}")
        return {"response": f"æŸ¥è©¢å¤±æ•—ï¼š{error_msg}"}


# ========================================
# Optimized RAG Functions
# ========================================

def get_vectorstore_cached():
    """Get cached vectorstore."""
    global _vectorstore_cache
    if _vectorstore_cache:
        return _vectorstore_cache
    
    _vectorstore_cache = prepare_vectorstore()
    return _vectorstore_cache

def prepare_vectorstore():
    """Prepare vectorstore (unchanged but cached)."""
    persist_directory = "./RAGè³‡è¨Š/vectorstore_db"
    populated_flag = os.path.join(persist_directory, ".populated")

    os.makedirs(persist_directory, exist_ok=True)

    vectorstore = Chroma(
        collection_name="full_documents", 
        embedding_function=OpenAIEmbeddings(),
        persist_directory=persist_directory
    )

    if not os.path.exists(populated_flag):
        folder_path = './RAGè³‡è¨Š'
        all_pdfname = []

        for filename in os.listdir(folder_path):
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(folder_path, filename)
                all_pdfname.append(pdf_path)

        for pdf_name in all_pdfname:
            pdf_text = extract_text_from_pdf(pdf_name)
            if not pdf_text.strip():
                continue

            child_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
            text_chunks = child_splitter.split_text(pdf_text)
            
            if not text_chunks:
                continue

            documents = [Document(page_content=chunk) for chunk in text_chunks if chunk.strip()]
            if documents:
                vectorstore.add_documents(documents)

        with open(populated_flag, "w") as f:
            f.write("populated")
        print("âœ… PDF content added to vector database.")
    else:
        print("ğŸ”„ Vector database already populated")

    return vectorstore

def extract_text_from_pdf(file_path):
    """Extract text from PDF (unchanged)."""
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text
    return text

async def handle_answer_intent_optimized(client, user_message):
    """Optimized RAG handler with reduced processing time."""
    vectorstore = get_vectorstore_cached()
    
    # Reduce initial retrieval
    top_chunks = retrieve_top_k_chunks(vectorstore, user_message, k=5)  # Reduced from 7
    print(f"Retrieved {len(top_chunks)} chunks")

    # Simplified chunk selection - just use top 2 chunks without complex scoring
    final_chunks = top_chunks[:2]  # Take top 2 directly
    print(f"Using top 2 chunks without scoring for speed")
    
    # Simplified summarization
    final_answer = summarize_chunks_optimized(client, user_message, final_chunks)
    
    return {"response": final_answer}

def retrieve_top_k_chunks(vectorstore, user_query, k=5):
    """Retrieve chunks (unchanged)."""
    sub_docs = vectorstore.similarity_search(user_query, k=k)
    return [doc.page_content for doc in sub_docs]

def summarize_chunks_optimized(client, query, chunks):
    """Optimized summarization with reduced complexity."""
    if not chunks:
        return "No relevant information found."
    
    # Read table content for context
    tables_path = "./CreateTables.txt"
    table_content = ""
    try:
        with open(tables_path, 'r', encoding='utf-8') as file:
            table_content = file.read()
    except Exception as e:
        print(f"Error reading table content: {e}")
        table_content = ""

    # Simplified prompt
    combined_content = "\n\n".join(chunks)
    
    # Create system prompt with table content if available
    system_prompt = "åŸºæ–¼æä¾›çš„å…§å®¹å›ç­”ç”¨æˆ¶å•é¡Œï¼Œä¿æŒç°¡æ½”æº–ç¢ºã€‚ä¸¦ä¸”**é¿å…**ç›´æ¥ä»¥è³‡æ–™åº«æ¬„ä½åç¨±é€²è¡Œå›ç­”ï¼Œè«‹ä»¥äººæ€§åŒ–çš„æ–¹å¼é€²è¡Œå›ç­”"
    if table_content:
        system_prompt += f"\n\nè³‡æ–™åº«çµæ§‹åƒè€ƒï¼š\n{table_content}"  # Limit table content size
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Faster model
            temperature=0.1,
            max_tokens=400,  # Limit response length
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"å•é¡Œï¼š{query}\n\nå…§å®¹ï¼š{combined_content}"}
            ]
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        print(f"Summarization error: {str(e)}")
        return "è™•ç†å›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹é‡è©¦ã€‚"

# ========================================
# Legacy compatibility functions (simplified)
# ========================================

def parse_database_records(records, column_names=None):
    """Legacy function (simplified)."""
    if not records:
        return []
    
    parsed_records = []
    for record in records:
        parsed_record = {}
        for i, value in enumerate(record):
            if isinstance(value, Decimal):
                parsed_record[f'col_{i}'] = float(value)
            elif isinstance(value, datetime):
                parsed_record[f'col_{i}'] = value.strftime('%Y-%m-%d %H:%M:%S')
            else:
                parsed_record[f'col_{i}'] = str(value) if value is not None else None
        parsed_records.append(parsed_record)
    
    return parsed_records

def get_tables_content():
    """Get cached tables content."""
    enhancer = SQLQueryEnhancer(None)
    return enhancer.get_enhanced_tables_content()