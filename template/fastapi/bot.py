from openai import OpenAI  # Import the OpenAI class
import os
from dotenv import load_dotenv  # Import for loading environment variables
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool  # For running synchronous code
from connect.connect import connectDB  # Import the custom connect function
from PyPDF2 import PdfReader
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document  # Import Document class
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
import re
from collections import Counter

# Load environment variables
load_dotenv()

# Initialize FastAPI router
botapi = APIRouter(tags=["bot"])

# Define a request model for the incoming data
class MessageRequest(BaseModel):
    message: str

@botapi.post("/botapi")
async def botmessage(request: MessageRequest):

    # Set the OpenAI API key
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))  # Pass the API key directly here
    if not client:
        raise HTTPException(status_code=500, detail="OpenAI API key is not set in the environment.")
    
    # If intent is query, proceed to generate the query
    tables_path = "./CreateTables.txt"
    with open(tables_path, 'r', encoding='utf-8') as file:
        tables_content = file.read()

    user_message = request.message

    intent_response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": f"""
You are an intelligent assistant that replies to the user's questions with precision. 
Your primary focus is to understand the user's intent before responding. 

You have the following tools:
- query: Use this when the user asks for information that requires external sources or research, you have these table: {tables_content}
- answer: Use this when the user asks about Carbon Footprint Verification, including its process, standards, calculations, or related details.
- others: Use this for any other questions or requests unrelated to Carbon Footprint Verification or external queries.

Your task is to:
1. **Accurately determine the user's intent** and respond with one of the following intents ONLY:
    - query
    - answer
    - others
            """},
            {"role": "user", "content": user_message},
        ]
    )
    
    # Extract intent from the response
    intent = intent_response.choices[0].message.content.strip().lower()
    print(f"Intent: {intent}")

    # Check the determined intent
    if intent == "query":
        # If intent is query, proceed to generate the query
        tables_path = "./CreateTables.txt"
        with open(tables_path, 'r', encoding='utf-8') as file:
            tables_content = file.read()
        
        query_intent = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"""
                æˆ‘å€‘çš„è³‡æ–™åº«æœ‰ä»¥ä¸‹tableï¼Œè«‹åˆ¤æ–·ä½¿ç”¨è€…çš„å•é¡Œå±¬æ–¼å“ªå€‹tableï¼Œä¸¦çµ¦å‡ºqueryçš„æŒ‡ä»¤
                (æ³¨æ„ï¼š
                1.åªè¦çµ¦å‡ºSQLæŒ‡ä»¤å³å¯ï¼Œä¸éœ€markdownæ ¼å¼
                2.æ¯æ¬¡éƒ½select * from table_nameå³å¯)
                {tables_content}
                """},
                {"role": "user", "content": user_message},
            ]
        )

        conn = connectDB()  # Establish connection using your custom connect function
        if conn:
            cursor = conn.cursor()
            # Secure SQL query using a parameterized query to prevent SQL injection
            cursor.execute(query_intent.choices[0].message.content)
            # Fetch all records
            records = cursor.fetchall()
            conn.close()

            result = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": f"""
                    ä¾æ“š{user_message}åœ¨{records}ä¸­å°‹æ‰¾ä½¿ç”¨è€…è¦çš„è³‡æ–™ä¸¦å›è¦†ï¼Œå¦‚æœæ²’ç‰¹åˆ¥è¦æ±‚å°±å›è¦†æ‰€æœ‰è³‡æ–™
                    """},
                ]
            )
            
            result_message = result.choices[0].message.content
            print(result_message)
            return {"response": result_message}
        else:
            print("Could not connect to the database.")
            return {"response": "Could not connect to the database."}

    elif intent == "answer":

        def extract_text_from_pdf(file_path):
            """Extracts text from a PDF file."""
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text
            return text

        persist_directory = "./RAGè³‡è¨Š/vectorstore_db"
        populated_flag = os.path.join(persist_directory, ".populated")

        folder_path = './RAGè³‡è¨Š'
        all_pdfname = []

        # Ensure the directory for ChromaDB exists
        os.makedirs(persist_directory, exist_ok=True)

        # 1. Initialize Vector Store
        vectorstore = Chroma(
            collection_name="full_documents", 
            embedding_function=OpenAIEmbeddings(),
            persist_directory=persist_directory
        )

        # 2. æº–å‚™æ‰€æœ‰ PDF æª”å
        for filename in os.listdir(folder_path):
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(folder_path, filename)
                all_pdfname.append(pdf_path)

        # 3. å¦‚æœå‘é‡è³‡æ–™åº«é‚„æ²’å»ºç½®ï¼Œå‰‡å°‡ PDF åŠ å…¥å‘é‡è³‡æ–™åº«
        if not os.path.exists(populated_flag):
            for pdf_name in all_pdfname:
                file_path = os.path.join(pdf_name)
                pdf_text = extract_text_from_pdf(file_path)

                if not pdf_text.strip():
                    print(f"âš ï¸ PDF '{pdf_name}' æ²’æœ‰æå–åˆ°ä»»ä½•æ–‡å­—ã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                    continue

                child_splitter = RecursiveCharacterTextSplitter(chunk_size=2500)
                text_chunks = child_splitter.split_text(pdf_text)

                if not text_chunks:
                    print(f"âš ï¸ PDF '{pdf_name}' çš„æ–‡å­—åˆ‡å‰²çµæœç‚ºç©ºã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                    continue

                documents = [Document(page_content=chunk) for chunk in text_chunks if chunk.strip()]

                if not documents:
                    print(f"âš ï¸ PDF '{pdf_name}' çš„ documents åˆ—è¡¨ç‚ºç©ºã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                    continue

                # åŠ å…¥å‘é‡è³‡æ–™åº«
                vectorstore.add_documents(documents)

            with open(populated_flag, "w") as f:
                f.write("populated")
            print("âœ… æ‰€æœ‰ PDF çš„å…§å®¹å·²åŠ å…¥å‘é‡è³‡æ–™åº«ã€‚")
        else:
            print("ğŸ”„ å‘é‡è³‡æ–™åº«å·²æœ‰è³‡æ–™")

        # ---- å‡½æ•¸å°è£ï¼šæª¢ç´¢ top-k chunks ----
        def retrieve_top_k_chunks(user_query, k=5):
            """
            å¾ vectorstore ä¸­æª¢ç´¢æœ€ç›¸é—œçš„ top-k chunksã€‚
            """
            sub_docs = vectorstore.similarity_search(user_query, k=k)
            return [doc.page_content for doc in sub_docs]

        # ---- å‡½æ•¸å°è£ï¼šPointwise è©•åˆ† (åŒæ­¥ç‰ˆæœ¬) ----
        def score_chunk_sync(client, query, chunk, index):
            """
            åŒæ­¥å‡½æ•¸ï¼šå°å–®å€‹ chunk é€²è¡Œè©•åˆ†
            """
            score_prompt = f"""
ä½ çš„ä»»å‹™æ˜¯è©•ä¼°ä»¥ä¸‹å…§å®¹ç‰‡æ®µèˆ‡ä½¿ç”¨è€…å•é¡Œçš„ç›¸é—œæ€§ã€‚
æ ¹æ“šç›¸é—œæ€§çµ¦è©²ç‰‡æ®µæ‰“åˆ†ï¼Œåˆ†æ•¸ç¯„åœæ˜¯0åˆ°10ï¼Œå…¶ä¸­10åˆ†è¡¨ç¤ºéå¸¸ç›¸é—œï¼Œ0åˆ†è¡¨ç¤ºå®Œå…¨ä¸ç›¸é—œã€‚

è©•åˆ†æ¨™æº–ï¼š
- å…§å®¹æ˜¯å¦ç›´æ¥å›ç­”å•é¡Œ (0-3åˆ†)
- å…§å®¹æ˜¯å¦æä¾›ç›¸é—œèƒŒæ™¯çŸ¥è­˜ (0-2åˆ†)
- å…§å®¹çš„å°ˆæ¥­æ€§å’Œæº–ç¢ºæ€§ (0-3åˆ†)
- å…§å®¹çš„å®Œæ•´æ€§ (0-2åˆ†)

åªéœ€è¼¸å‡ºä¸€å€‹æ•´æ•¸åˆ†æ•¸ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡‹æˆ–æ¨™é»ç¬¦è™Ÿï¼Œåªæœ‰æ•¸å­—ã€‚
ä¾‹å¦‚ï¼š7

ä½¿ç”¨è€…å•é¡Œ: {query}

å…§å®¹ç‰‡æ®µ:
{chunk}
            """

            score_response = client.chat.completions.create(
                model="gpt-4o",
                temperature=0,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­è©•åˆ†å“¡ã€‚ä½ çš„å›ç­”å¿…é ˆåªåŒ…å«ä¸€å€‹æ•¸å­—ï¼ˆ0-10ï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"},
                    {"role": "user", "content": score_prompt}
                ]
            )

            content = score_response.choices[0].message.content.strip()
            print(f"Chunk {index} raw score: {content}")

            try:
                # å˜—è©¦å°‡å›æ‡‰ç›´æ¥è½‰æ›ç‚ºæ•´æ•¸
                score = int(content)
                # ç¢ºä¿åˆ†æ•¸åœ¨ 0-10 çš„ç¯„åœå…§
                return max(0, min(score, 10)), chunk
            except Exception as e:
                print(f"Error parsing score for chunk {index}: {e}")
                # å¦‚æœç„¡æ³•è§£æç‚ºæ•´æ•¸ï¼Œå˜—è©¦å¾æ–‡æœ¬ä¸­æå–æ•¸å­—
                number_match = re.search(r'\d+', content)
                if number_match:
                    try:
                        score = int(number_match.group())
                        return max(0, min(score, 10)), chunk
                    except:
                        pass
                return 5, chunk  # é è¨­å›å‚³ä¸­é–“å€¼

        # ---- å‡½æ•¸å°è£ï¼šéåŒæ­¥é¸æ“‡æœ€ä½³ chunks ----
        async def select_best_chunks_async(client, query, chunks, top_n=3):
            """
            éåŒæ­¥å‡½æ•¸ï¼šå°æ¯å€‹ chunk é€²è¡Œç¨ç«‹è©•åˆ†ï¼Œé¸å‡ºåˆ†æ•¸æœ€é«˜çš„ top_n å€‹ã€‚
            """
            print(f"Scoring {len(chunks)} chunks asynchronously...")
            
            # ä½¿ç”¨ ThreadPoolExecutor è™•ç†å¯èƒ½é˜»å¡çš„ API å‘¼å«
            scored_chunks = []
            with ThreadPoolExecutor(max_workers=min(10, len(chunks))) as executor:
                # å»ºç«‹ä»»å‹™æ¸…å–®
                futures = []
                for i, chunk in enumerate(chunks):
                    future = executor.submit(score_chunk_sync, client, query, chunk, i)
                    futures.append(future)
                
                # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
                for i, future in enumerate(futures):
                    try:
                        score, chunk = future.result()
                        scored_chunks.append((score, chunk))
                        print(f"Completed {i+1}/{len(chunks)} evaluations")
                    except Exception as e:
                        print(f"Error in chunk evaluation {i}: {e}")
                        # å¦‚æœè©•åˆ†å¤±æ•—ï¼Œçµ¦äºˆä½åˆ†æ•¸ä½†ä»ä¿ç•™é€™å€‹ chunk
                        scored_chunks.append((1, chunks[i]))
            
            # æŒ‰åˆ†æ•¸æ’åºä¸¦å– top_n å€‹
            scored_chunks.sort(key=lambda x: x[0], reverse=True)  # å¾é«˜åˆ°ä½æ’åº
            top_chunks = [chunk for _, chunk in scored_chunks[:top_n]]
            
            return top_chunks

        # ---- å‡½æ•¸å°è£ï¼šç¸½çµ ----
        def summarize_chunks(query, chunks):
            """
            å°é¸å‡ºçš„ chunks é€²è¡Œç¸½çµï¼Œç”Ÿæˆæœ€çµ‚å›ç­”ã€‚
            """
            summarize_response = client.chat.completions.create(
                model="gpt-4o",
                temperature=0,  # è®“æ¨¡å‹å›è¦†ç›¡å¯èƒ½ä¸€è‡´
                messages=[
                    {
                        "role": "system",
                        "content": f"ç”¨#zh-twå›ç­”ï¼Œæ ¹æ“šæ‚¨çš„å•é¡Œ({query})ï¼Œä»¥ä¸‹æ˜¯å¾ç›¸é—œå…§å®¹ä¸­æå–çš„ç¸½çµï¼š"
                    },
                    {
                        "role": "user",
                        "content": "\n\n".join(chunks)
                    },
                ]
            )
            return summarize_response.choices[0].message.content

        # ---- main process ----
        query = user_message

        # 1. æª¢ç´¢ top-k chunks
        top_chunks = retrieve_top_k_chunks(query, k=7)
        print(f"Step1: Retrieved top 7 chunks, number of chunks: {len(top_chunks)}")

        # 2. Pointwise è©•åˆ†ï¼Œé¸å‡ºæœ€ä½³çš„ chunks
        final_chunks = await select_best_chunks_async(client, query, top_chunks, top_n=3)
        print(f"Step2: Selected top 3 chunks based on pointwise scoring")

        # 3. å°ç•™ä¸‹çš„ chunk é€²è¡Œç¸½çµ
        final_answer = summarize_chunks(query, final_chunks)

        return {"response": final_answer}
    else:
        # Handle other intents here
        print("Intent is others.")
        return {"response": "æŠ±æ­‰ï¼Œç¢³æ™ºéƒåƒ…èƒ½å›ç­”è³‡æ–™åº«ä¸­å’Œç¢³ç›¤æŸ¥ç›¸é—œçš„å•é¡Œ"}