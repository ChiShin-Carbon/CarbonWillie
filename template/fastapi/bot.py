from openai import OpenAI
import os
from dotenv import load_dotenv
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool
from connect.connect import connectDB
from PyPDF2 import PdfReader
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
import re
from collections import Counter

# Load environment variables
load_dotenv()

# Initialize FastAPI router
botapi = APIRouter(tags=["bot"])

# Define a request model for the incoming data
class MessageRequest(BaseModel):
    message: str


@botapi.post("/botapi")
async def botmessage(request: MessageRequest):
    # Set the OpenAI API key
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    if not client:
        raise HTTPException(status_code=500, detail="OpenAI API key is not set in the environment.")
    
    # Get user message
    user_message = request.message

    # Determine intent
    intent_response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": f"""
You are an intelligent assistant that replies to the user's questions with precision. 
Your primary focus is to understand the user's intent before responding. 

You have the following tools:
- query: Use this when the user asks for information that requires external sources or research, you have these table: {get_tables_content()}
- answer: Use this when the user asks about Carbon Footprint Verification, including its process, standards, calculations, or related details.
- others: Use this for any other questions or requests unrelated to Carbon Footprint Verification or external queries.

Your task is to:
1. **Accurately determine the user's intent** and respond with one of the following intents ONLY:
    - query
    - answer
    - others
            """},
            {"role": "user", "content": user_message},
        ]
    )
    
    # Extract intent from the response
    intent = intent_response.choices[0].message.content.strip().lower()
    print(f"Intent: {intent}")

    # Process based on intent
    if intent == "query":
        return await handle_query_intent(client, user_message)
    elif intent == "answer":
        return await handle_answer_intent(client, user_message)
    else:
        # Handle other intents
        print("Intent is others.")
        return {"response": "æŠ±æ­‰ï¼Œç¢³æ™ºéƒåƒ…èƒ½å›ç­”è³‡æ–™åº«ä¸­å’Œç¢³ç›¤æŸ¥ç›¸é—œçš„å•é¡Œ"}


def get_tables_content():
    """Read and return database tables information."""
    tables_path = "./CreateTables.txt"
    with open(tables_path, 'r', encoding='utf-8') as file:
        return file.read()


async def handle_query_intent(client, user_message):
    """Handle database query intent."""
    tables_content = get_tables_content()
    
    try:
        # Generate SQL query
        query_intent = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"""
                æˆ‘å€‘çš„è³‡æ–™åº«æœ‰ä»¥ä¸‹tableï¼Œè«‹åˆ¤æ–·ä½¿ç”¨è€…çš„å•é¡Œå±¬æ–¼å“ªå€‹tableï¼Œä¸¦çµ¦å‡ºqueryçš„æŒ‡ä»¤
                (æ³¨æ„ï¼š
                1.åªè¦çµ¦å‡ºSQLæŒ‡ä»¤å³å¯ï¼Œä¸éœ€markdownæ ¼å¼
                2.æ¯æ¬¡éƒ½select * from table_nameå³å¯(é¸æ“‡æ•´å€‹table)
                3.é¿å…ä½¿ç”¨ä¸­æ–‡æ¨™é»ç¬¦è™Ÿå¦‚ï¼šï¼Œã€ï¼›ç­‰ï¼Œè«‹ä½¿ç”¨è‹±æ–‡æ¨™é»ç¬¦è™Ÿ)
                {tables_content}
                """},
                {"role": "user", "content": user_message},
            ]
        )

        # Sanitize the SQL query - replace Chinese punctuation with English
        sql_query = query_intent.choices[0].message.content
        sql_query = sql_query.replace('ï¼Œ', ',').replace('ï¼›', ';').replace('ï¼š', ':')
        
        # Execute database query
        conn = connectDB()
        if conn:
            try:
                cursor = conn.cursor()
                cursor.execute(sql_query)
                records = cursor.fetchall()
                
                # Generate response based on query results
                result = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": f"""
                        ä¾æ“š{user_message}åœ¨{records}ä¸­å°‹æ‰¾ä½¿ç”¨è€…è¦çš„è³‡æ–™ä¸¦å›è¦†ï¼Œå¦‚æœæ²’ç‰¹åˆ¥è¦æ±‚å°±å›è¦†æ‰€æœ‰è³‡æ–™
                        """},
                    ]
                )
                
                result_message = result.choices[0].message.content
                print(result_message)
                return {"response": result_message}
            except Exception as e:
                print(f"Database execution error: {e}")
                return {"response": "æŠ±æ­‰ï¼Œç¢³æ™ºéƒåƒ…èƒ½å›ç­”è³‡æ–™åº«ä¸­å’Œç¢³ç›¤æŸ¥ç›¸é—œçš„å•é¡Œ"}
            finally:
                conn.close()
        else:
            print("Could not connect to the database.")
            return {"response": "æŠ±æ­‰ï¼Œç¢³æ™ºéƒåƒ…èƒ½å›ç­”è³‡æ–™åº«ä¸­å’Œç¢³ç›¤æŸ¥ç›¸é—œçš„å•é¡Œ"}
    except Exception as e:
        print(f"Error in handling query intent: {e}")
        return {"response": "æŠ±æ­‰ï¼Œç¢³æ™ºéƒåƒ…èƒ½å›ç­”è³‡æ–™åº«ä¸­å’Œç¢³ç›¤æŸ¥ç›¸é—œçš„å•é¡Œ"}

async def handle_answer_intent(client, user_message):
    """Handle RAG-based answer intent using PDF documents."""
    # Initialize and prepare vector store
    vectorstore = prepare_vectorstore()
    
    # Detect user language
    language = detect_user_language(client, user_message)
    print(f"Detected language: {language}")
    
    # 1. Retrieve top-k chunks
    top_chunks = retrieve_top_k_chunks(vectorstore, user_message, k=7)
    print(f"Step1: Retrieved top 7 chunks, number of chunks: {len(top_chunks)}")

    # 2. Pointwise scoring, select best chunks
    final_chunks = await select_best_chunks_async(client, user_message, top_chunks, top_n=3)
    print(f"Step2: Selected top 3 chunks based on pointwise scoring")

    # 3. Summarize selected chunks with detected language
    final_answer = summarize_chunks(client, user_message, final_chunks, language=language)

    return {"response": final_answer}


def detect_user_language(client, user_message):
    """Detect the user's language from their message."""
    # Use a language detection prompt with the OpenAI API
    language_detection = client.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": """
            Detect the language of the user's message and respond with only the language code.
            Use standard language codes like:
            - 'en' for English
            - 'zh-tw' for Traditional Chinese
            - 'ja' for Japanese
            - etc.
            
            Only return the language code, no other text.
            """},
            {"role": "user", "content": user_message}
        ]
    )
    
    language_code = language_detection.choices[0].message.content.strip().lower()
    
    # Validate language code (basic validation)
    if not language_code or len(language_code) > 10:
        # Default to English if detection fails
        return "en"
    
    return language_code

def prepare_vectorstore():
    """Prepare and return the vector store for RAG."""
    persist_directory = "./RAGè³‡è¨Š/vectorstore_db"
    populated_flag = os.path.join(persist_directory, ".populated")

    # Ensure the directory for ChromaDB exists
    os.makedirs(persist_directory, exist_ok=True)

    # Initialize Vector Store
    vectorstore = Chroma(
        collection_name="full_documents", 
        embedding_function=OpenAIEmbeddings(),
        persist_directory=persist_directory
    )

    # If vector database not populated, add PDF content
    if not os.path.exists(populated_flag):
        folder_path = './RAGè³‡è¨Š'
        all_pdfname = []

        # Prepare all PDF filenames
        for filename in os.listdir(folder_path):
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(folder_path, filename)
                all_pdfname.append(pdf_path)

        # Add PDFs to vector database
        for pdf_name in all_pdfname:
            file_path = os.path.join(pdf_name)
            pdf_text = extract_text_from_pdf(file_path)

            if not pdf_text.strip():
                print(f"âš ï¸ PDF '{pdf_name}' æ²’æœ‰æå–åˆ°ä»»ä½•æ–‡å­—ã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                continue

            child_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
            text_chunks = child_splitter.split_text(pdf_text)

            if not text_chunks:
                print(f"âš ï¸ PDF '{pdf_name}' çš„æ–‡å­—åˆ‡å‰²çµæœç‚ºç©ºã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                continue

            documents = [Document(page_content=chunk) for chunk in text_chunks if chunk.strip()]

            if not documents:
                print(f"âš ï¸ PDF '{pdf_name}' çš„ documents åˆ—è¡¨ç‚ºç©ºã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                continue

            # Add to vector database
            vectorstore.add_documents(documents)

        with open(populated_flag, "w") as f:
            f.write("populated")
        print("âœ… æ‰€æœ‰ PDF çš„å…§å®¹å·²åŠ å…¥å‘é‡è³‡æ–™åº«ã€‚")
    else:
        print("ğŸ”„ å‘é‡è³‡æ–™åº«å·²æœ‰è³‡æ–™")

    return vectorstore


def extract_text_from_pdf(file_path):
    """Extract text from a PDF file."""
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text
    return text


def retrieve_top_k_chunks(vectorstore, user_query, k=5):
    """Retrieve the top-k most relevant chunks from the vector store."""
    sub_docs = vectorstore.similarity_search(user_query, k=k)
    return [doc.page_content for doc in sub_docs]


def score_chunk_sync(client, query, chunk, index):
    """Score a single chunk for relevance to the query."""
    score_prompt = f"""
è©•ä¼°ä»»å‹™ï¼šå°å…§å®¹ç‰‡æ®µèˆ‡ä½¿ç”¨è€…å•é¡Œçš„ç›¸é—œæ€§é€²è¡Œé‡åŒ–è©•åˆ†

ä½¿ç”¨è€…å•é¡Œ: {query}

å¾…è©•ä¼°å…§å®¹ç‰‡æ®µ:
{chunk}

è©•åˆ†æ¨™æº–ï¼ˆç¸½åˆ†10åˆ†ï¼‰:
1. ç›´æ¥å›ç­”å•é¡Œ (0-3åˆ†)
   - 3åˆ†ï¼šå®Œå…¨ä¸”ç›´æ¥å›ç­”å•é¡Œæ ¸å¿ƒ
   - 2åˆ†ï¼šéƒ¨åˆ†å›ç­”å•é¡Œ
   - 1åˆ†ï¼šåƒ…æåŠå•é¡Œä½†æœªç›´æ¥å›ç­”
   - 0åˆ†ï¼šå®Œå…¨æœªå›ç­”å•é¡Œ

2. ç›¸é—œèƒŒæ™¯çŸ¥è­˜ (0-2åˆ†)
   - 2åˆ†ï¼šæä¾›è±å¯Œä¸”å¿…è¦çš„èƒŒæ™¯è³‡è¨Š
   - 1åˆ†ï¼šæä¾›ä¸€äº›ç›¸é—œèƒŒæ™¯
   - 0åˆ†ï¼šæœªæä¾›ç›¸é—œèƒŒæ™¯

3. å°ˆæ¥­æ€§å’Œæº–ç¢ºæ€§ (0-3åˆ†)
   - 3åˆ†ï¼šå…§å®¹å°ˆæ¥­ã€æº–ç¢ºä¸”æœ‰æ·±åº¦
   - 2åˆ†ï¼šå…§å®¹å¤§è‡´æº–ç¢ºä½†ç¼ºä¹æ·±åº¦
   - 1åˆ†ï¼šå…§å®¹æœ‰è¼•å¾®éŒ¯èª¤æˆ–éæ–¼ç°¡åŒ–
   - 0åˆ†ï¼šå…§å®¹æœ‰æ˜é¡¯éŒ¯èª¤æˆ–èª¤å°

4. å®Œæ•´æ€§ (0-2åˆ†)
   - 2åˆ†ï¼šå…§å®¹å®Œæ•´ï¼Œç„¡éœ€é¡å¤–è³‡è¨Š
   - 1åˆ†ï¼šå…§å®¹åŸºæœ¬å®Œæ•´ä½†æœ‰éºæ¼
   - 0åˆ†ï¼šå…§å®¹ç‰‡æ®µä¸”ä¸å®Œæ•´

æ€è€ƒæ­¥é©Ÿï¼š
1. ä»”ç´°é–±è®€ä½¿ç”¨è€…å•é¡Œï¼Œç¢ºå®šæ ¸å¿ƒéœ€æ±‚
2. é€æ¢è©•ä¼°å…§å®¹ç‰‡æ®µåœ¨å„æ¨™æº–ä¸‹çš„è¡¨ç¾
3. ç‚ºæ¯å€‹æ¨™æº–åˆ†é…é©ç•¶åˆ†æ•¸
4. è¨ˆç®—ç¸½åˆ†

åªè¼¸å‡ºæœ€çµ‚ç¸½åˆ†ï¼ˆ0-10çš„æ•´æ•¸ï¼‰ã€‚ä¸è¦åŒ…å«ä»»ä½•æ–‡å­—ã€è§£é‡‹æˆ–åˆ†æéç¨‹ã€‚
"""

    score_response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ç²¾ç¢ºçš„å…§å®¹è©•åˆ†å°ˆå®¶ã€‚ä½ å¿…é ˆéµå¾ªæŒ‡ç¤ºï¼Œåªå›å‚³ä¸€å€‹0-10ä¹‹é–“çš„æ•´æ•¸ä½œç‚ºè©•åˆ†çµæœï¼Œä¸èƒ½æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€æ¨™é»æˆ–èªªæ˜ã€‚é•åæ­¤è¦å‰‡å°‡å°è‡´è©•åˆ†ç³»çµ±å¤±æ•ˆã€‚"},
            {"role": "user", "content": score_prompt}
        ]
    )

    content = score_response.choices[0].message.content.strip()
    print(f"Chunk {index} raw score: {content}")

    try:
        # Try to convert response to integer
        score = int(content)
        # Ensure score is in 0-10 range
        return max(0, min(score, 10)), chunk
    except Exception as e:
        print(f"Error parsing score for chunk {index}: {e}")
        # If parsing fails, try to extract number from text
        number_match = re.search(r'\d+', content)
        if number_match:
            try:
                score = int(number_match.group())
                return max(0, min(score, 10)), chunk
            except:
                pass
        return 5, chunk  # Default to middle value


async def select_best_chunks_async(client, query, chunks, top_n=3):
    """Asynchronously score chunks and select the best ones."""
    print(f"Scoring {len(chunks)} chunks asynchronously...")
    
    # Use ThreadPoolExecutor for potentially blocking API calls
    scored_chunks = []
    with ThreadPoolExecutor(max_workers=min(10, len(chunks))) as executor:
        # Create task list
        futures = []
        for i, chunk in enumerate(chunks):
            future = executor.submit(score_chunk_sync, client, query, chunk, i)
            futures.append(future)
        
        # Wait for all tasks to complete
        for i, future in enumerate(futures):
            try:
                score, chunk = future.result()
                scored_chunks.append((score, chunk))
                print(f"Completed {i+1}/{len(chunks)} evaluations")
            except Exception as e:
                print(f"Error in chunk evaluation {i}: {e}")
                # If scoring fails, give low score but keep the chunk
                scored_chunks.append((1, chunks[i]))
    
    # Sort by score and take top_n
    scored_chunks.sort(key=lambda x: x[0], reverse=True)  # Sort from high to low
    top_chunks = [chunk for _, chunk in scored_chunks[:top_n]]
    
    return top_chunks


def summarize_chunks(client, query, chunks, language=None):
    """
    Summarize relevant chunks to generate a final answer.
    
    Args:
        client: OpenAI client
        query (str): The user's question
        chunks (list): List of relevant text segments
        language (str, optional): Preferred language code (e.g., 'en', 'zh-tw') according to user's language
            
    Returns:
        str: Summarized response in markdown format based on the chunks
    """
    # Handle empty chunks case
    if not chunks:
        return "No relevant information found to answer your query."
    
    # Prepare language instruction
    lang_instruction = ""
    if language:
        lang_instruction = f"Please respond in {language}. "
    
    # Create system prompt
    system_prompt = (
        f"{lang_instruction}Based on the user's question: '{query}', "
        "synthesize the following relevant information into a comprehensive answer. "
        "Maintain factual accuracy and cite information directly from the provided content."
    )
    
    try:
        # Combine chunks with proper separation and context
        formatted_chunks = "\n\n--- CHUNK START ---\n" + "\n--- CHUNK END ---\n\n--- CHUNK START ---\n".join(chunks) + "\n--- CHUNK END ---"
        
        # Make API call with error handling
        summarize_response = client.chat.completions.create(
            model="gpt-4o",
            temperature=0.2,  # Slight randomness for more natural responses while maintaining consistency
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": formatted_chunks}
            ],
            max_tokens=1024  # Set reasonable output length limit
        )
        
        return summarize_response.choices[0].message.content
    
    except Exception as e:
        # Log error and return fallback response
        print(f"Error in summarization: {str(e)}")
        return "Sorry, I encountered an issue while processing your request. Please try again."