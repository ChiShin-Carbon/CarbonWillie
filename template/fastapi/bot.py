from openai import OpenAI
import os
from dotenv import load_dotenv
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool
from connect.connect import connectDB
from PyPDF2 import PdfReader
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
import re
from collections import Counter
from datetime import datetime
from decimal import Decimal

# Load environment variables
load_dotenv()

# Initialize FastAPI router
botapi = APIRouter(tags=["bot"])

# Define a request model for the incoming data
class MessageRequest(BaseModel):
    message: str


@botapi.post("/botapi")
async def botmessage(request: MessageRequest):
    # Set the OpenAI API key
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    if not client:
        raise HTTPException(status_code=500, detail="OpenAI API key is not set in the environment.")
    
    # Get user message
    user_message = request.message

    # Determine intent
    intent_response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": f"""
You are an intelligent assistant that replies to the user's questions with precision. 
Your primary focus is to understand the user's intent before responding. 

You have the following tools:
- query: Use this when the user asks for information that requires external sources or research, you have these table: {get_tables_content()}
- answer: Use this when the user asks about Carbon Footprint Verification, including its process, standards, calculations, or related details.
- others: Use this for any other questions or requests unrelated to Carbon Footprint Verification or external queries.

Your task is to:
1. **Accurately determine the user's intent** and respond with one of the following intents ONLY:
    - query
    - answer
    - others
            """},
            {"role": "user", "content": user_message},
        ]
    )
    
    # Extract intent from the response
    intent = intent_response.choices[0].message.content.strip().lower()
    print(f"Intent: {intent}")

    # Process based on intent
    if intent == "query":
        return await handle_query_intent(client, user_message)
    elif intent == "answer":
        return await handle_answer_intent(client, user_message)
    else:
        # Handle other intents
        print("Intent is others.")
        return {"response": "æŠ±æ­‰ï¼Œç¢³æ™ºéƒåƒ…èƒ½å›ç­”è³‡æ–™åº«ä¸­å’Œç¢³ç›¤æŸ¥ç›¸é—œçš„å•é¡Œ"}


def get_tables_content():
    """Read and return database tables information."""
    tables_path = "./CreateTables.txt"
    with open(tables_path, 'r', encoding='utf-8') as file:
        return file.read()


def parse_database_records(records, column_names=None):
    """
    Parse database records into a more readable format.
    
    Args:
        records: Raw database records (list of tuples)
        column_names: List of column names (optional)
    
    Returns:
        List of dictionaries with parsed data
    """
    if not records:
        return []
    
    parsed_records = []
    for record in records:
        parsed_record = {}
        for i, value in enumerate(record):
            # Convert different data types to readable format
            if isinstance(value, Decimal):
                parsed_record[f'col_{i}'] = float(value)
            elif isinstance(value, datetime):
                parsed_record[f'col_{i}'] = value.strftime('%Y-%m-%d %H:%M:%S')
            elif hasattr(value, 'date') and callable(getattr(value, 'date')):  # datetime.date
                parsed_record[f'col_{i}'] = value.strftime('%Y-%m-%d')
            else:
                parsed_record[f'col_{i}'] = str(value) if value is not None else None
        
        parsed_records.append(parsed_record)
    
    return parsed_records


def get_column_info(cursor, table_name):
    """
    Get column information for a table to better understand the data structure.
    
    Args:
        cursor: Database cursor
        table_name: Name of the table
    
    Returns:
        List of column names
    """
    try:
        # Try to get column information (this syntax might vary depending on your database)
        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = cursor.fetchall()
        return [col[1] for col in columns]  # Column names are usually in index 1
    except:
        # If PRAGMA doesn't work, try alternative method
        try:
            cursor.execute(f"SELECT * FROM {table_name} LIMIT 0")
            return [description[0] for description in cursor.description]
        except:
            return None


async def handle_query_intent(client, user_message):
    """Handle database query intent with improved answer generation."""
    tables_content = get_tables_content()
    
    try:
        # Generate SQL query with better context
        query_intent = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"""
                æˆ‘å€‘çš„è³‡æ–™åº«æœ‰ä»¥ä¸‹tableï¼Œè«‹æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œç”Ÿæˆé©ç•¶çš„SQLæŸ¥è©¢ã€‚
                
                é‡è¦æŒ‡ç¤ºï¼š
                1. åªå›å‚³SQLæŸ¥è©¢èªå¥ï¼Œä¸éœ€è¦markdownæ ¼å¼
                2. æ ¹æ“šå•é¡Œçš„å…·é«”éœ€æ±‚é¸æ“‡é©ç•¶çš„æ¬„ä½ï¼Œä¸ä¸€å®šè¦select *
                3. å¦‚æœå•é¡Œæ¶‰åŠç‰¹å®šå¹´ä»½ã€æ—¥æœŸæˆ–æ¢ä»¶ï¼Œè«‹åœ¨WHEREå­å¥ä¸­åŒ…å«é€™äº›æ¢ä»¶
                4. é¿å…ä½¿ç”¨ä¸­æ–‡æ¨™é»ç¬¦è™Ÿï¼Œä½¿ç”¨è‹±æ–‡æ¨™é»ç¬¦è™Ÿ
                5. å¦‚æœéœ€è¦èšåˆæ•¸æ“šï¼ˆå¦‚ç¸½å’Œã€å¹³å‡å€¼ã€è¨ˆæ•¸ï¼‰ï¼Œè«‹ä½¿ç”¨é©ç•¶çš„èšåˆå‡½æ•¸
                
                è³‡æ–™åº«çµæ§‹ï¼š
                {tables_content}
                """},
                {"role": "user", "content": user_message},
            ]
        )

        # Sanitize the SQL query
        sql_query = query_intent.choices[0].message.content.strip()
        sql_query = sql_query.replace('ï¼Œ', ',').replace('ï¼›', ';').replace('ï¼š', ':')
        
        # Remove markdown formatting if present
        sql_query = re.sub(r'```sql\n?|```\n?', '', sql_query)
        
        print(f"Generated SQL: {sql_query}")
        
        # Execute database query
        conn = connectDB()
        if conn:
            try:
                cursor = conn.cursor()
                cursor.execute(sql_query)
                records = cursor.fetchall()
                
                # Get column names for better context
                column_names = [description[0] for description in cursor.description] if cursor.description else None
                
                # Parse records for better readability
                parsed_records = parse_database_records(records, column_names)
                
                # Generate a more user-friendly response
                response_prompt = f"""
                ä½¿ç”¨è€…å•é¡Œï¼š{user_message}
                
                è³‡æ–™åº«æŸ¥è©¢çµæœï¼š
                æ¬„ä½åç¨±ï¼š{column_names if column_names else 'æœªçŸ¥'}
                è³‡æ–™ç­†æ•¸ï¼š{len(records)}
                è³‡æ–™å…§å®¹ï¼š{parsed_records if parsed_records else records}
                
                è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šï¼Œä»¥è‡ªç„¶ã€æ˜“æ‡‚çš„æ–¹å¼å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚è¦æ±‚ï¼š
                1. ç›´æ¥å›ç­”å•é¡Œï¼Œé¿å…é¡¯ç¤ºåŸå§‹è³‡æ–™åº«è¨˜éŒ„æ ¼å¼
                2. å°‡æ•¸æ“šè½‰æ›ç‚ºæœ‰æ„ç¾©çš„è³‡è¨Š
                3. å¦‚æœæœ‰å¤šç­†ç›¸åŒçš„è³‡æ–™ï¼Œè«‹é€²è¡Œé©ç•¶çš„ç¸½çµ
                4. ä½¿ç”¨æ¸…æ™°çš„ä¸­æ–‡è¡¨é”
                5. å¦‚æœæ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼Œè«‹æ˜ç¢ºèªªæ˜
                6. é¿å…é¡¯ç¤ºæŠ€è¡“æ€§çš„è³‡æ–™åº«æ¬„ä½åç¨±å’ŒåŸå§‹tupleæ ¼å¼
                """
                
                result = client.chat.completions.create(
                    model="gpt-4o",
                    temperature=0.1,  # Lower temperature for more consistent responses
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ•¸æ“šåˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤ å°‡è³‡æ–™åº«æŸ¥è©¢çµæœè½‰æ›ç‚ºæ¸…æ™°ã€æ˜“æ‡‚çš„å›ç­”ã€‚è«‹é¿å…é¡¯ç¤ºåŸå§‹çš„è³‡æ–™åº«è¨˜éŒ„æ ¼å¼ï¼Œè€Œæ˜¯æä¾›æœ‰æ„ç¾©çš„è³‡è¨Šæ‘˜è¦ã€‚"},
                        {"role": "user", "content": response_prompt}
                    ]
                )
                
                result_message = result.choices[0].message.content
                print(f"Generated response: {result_message}")
                return {"response": result_message}
                
            except Exception as e:
                print(f"Database execution error: {e}")
                # Provide more specific error handling
                if "no such table" in str(e).lower():
                    return {"response": "æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°ç›¸é—œçš„è³‡æ–™è¡¨ã€‚è«‹ç¢ºèªæ‚¨çš„å•é¡Œæ˜¯å¦èˆ‡è³‡æ–™åº«ä¸­çš„è³‡æ–™ç›¸é—œã€‚"}
                elif "syntax error" in str(e).lower():
                    return {"response": "æŠ±æ­‰ï¼ŒæŸ¥è©¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹å˜—è©¦é‡æ–°è¡¨è¿°æ‚¨çš„å•é¡Œã€‚"}
                else:
                    return {"response": "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹ç¨å¾Œå†è©¦æˆ–é‡æ–°è¡¨è¿°æ‚¨çš„å•é¡Œã€‚"}
            finally:
                conn.close()
        else:
            print("Could not connect to the database.")
            return {"response": "æŠ±æ­‰ï¼Œç„¡æ³•é€£æ¥åˆ°è³‡æ–™åº«ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"}
            
    except Exception as e:
        print(f"Error in handling query intent: {e}")
        return {"response": "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"}


async def handle_answer_intent(client, user_message):
    """Handle RAG-based answer intent using PDF documents."""
    # Initialize and prepare vector store
    vectorstore = prepare_vectorstore()
    
    # Detect user language
    language = detect_user_language(client, user_message)
    print(f"Detected language: {language}")
    
    # 1. Retrieve top-k chunks
    top_chunks = retrieve_top_k_chunks(vectorstore, user_message, k=7)
    print(f"Step1: Retrieved top 7 chunks, number of chunks: {len(top_chunks)}")

    # 2. Pointwise scoring, select best chunks
    final_chunks = await select_best_chunks_async(client, user_message, top_chunks, top_n=3)
    print(f"Step2: Selected top 3 chunks based on pointwise scoring")

    # 3. Summarize selected chunks with detected language
    final_answer = summarize_chunks(client, user_message, final_chunks, language=language)

    return {"response": final_answer}


def detect_user_language(client, user_message):
    """Detect the user's language from their message."""
    # Use a language detection prompt with the OpenAI API
    language_detection = client.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": """
            Detect the language of the user's message and respond with only the language code.
            Use standard language codes like:
            - 'en' for English
            - 'zh-tw' for Traditional Chinese
            - 'ja' for Japanese
            - etc.
            
            Only return the language code, no other text.
            """},
            {"role": "user", "content": user_message}
        ]
    )
    
    language_code = language_detection.choices[0].message.content.strip().lower()
    
    # Validate language code (basic validation)
    if not language_code or len(language_code) > 10:
        # Default to English if detection fails
        return "en"
    
    return language_code

def prepare_vectorstore():
    """Prepare and return the vector store for RAG."""
    persist_directory = "./RAGè³‡è¨Š/vectorstore_db"
    populated_flag = os.path.join(persist_directory, ".populated")

    # Ensure the directory for ChromaDB exists
    os.makedirs(persist_directory, exist_ok=True)

    # Initialize Vector Store
    vectorstore = Chroma(
        collection_name="full_documents", 
        embedding_function=OpenAIEmbeddings(),
        persist_directory=persist_directory
    )

    # If vector database not populated, add PDF content
    if not os.path.exists(populated_flag):
        folder_path = './RAGè³‡è¨Š'
        all_pdfname = []

        # Prepare all PDF filenames
        for filename in os.listdir(folder_path):
            if filename.endswith(".pdf"):
                pdf_path = os.path.join(folder_path, filename)
                all_pdfname.append(pdf_path)

        # Add PDFs to vector database
        for pdf_name in all_pdfname:
            file_path = os.path.join(pdf_name)
            pdf_text = extract_text_from_pdf(file_path)

            if not pdf_text.strip():
                print(f"âš ï¸ PDF '{pdf_name}' æ²’æœ‰æå–åˆ°ä»»ä½•æ–‡å­—ã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                continue

            child_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
            text_chunks = child_splitter.split_text(pdf_text)

            if not text_chunks:
                print(f"âš ï¸ PDF '{pdf_name}' çš„æ–‡å­—åˆ‡å‰²çµæœç‚ºç©ºã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                continue

            documents = [Document(page_content=chunk) for chunk in text_chunks if chunk.strip()]

            if not documents:
                print(f"âš ï¸ PDF '{pdf_name}' çš„ documents åˆ—è¡¨ç‚ºç©ºã€‚è·³éæ­¤æª”æ¡ˆã€‚")
                continue

            # Add to vector database
            vectorstore.add_documents(documents)

        with open(populated_flag, "w") as f:
            f.write("populated")
        print("âœ… æ‰€æœ‰ PDF çš„å…§å®¹å·²åŠ å…¥å‘é‡è³‡æ–™åº«ã€‚")
    else:
        print("ğŸ”„ å‘é‡è³‡æ–™åº«å·²æœ‰è³‡æ–™")

    return vectorstore


def extract_text_from_pdf(file_path):
    """Extract text from a PDF file."""
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text
    return text


def retrieve_top_k_chunks(vectorstore, user_query, k=5):
    """Retrieve the top-k most relevant chunks from the vector store."""
    sub_docs = vectorstore.similarity_search(user_query, k=k)
    return [doc.page_content for doc in sub_docs]


def score_chunk_sync(client, query, chunk, index):
    """Score a single chunk for relevance to the query."""
    score_prompt = f"""
è©•ä¼°ä»»å‹™ï¼šå°å…§å®¹ç‰‡æ®µèˆ‡ä½¿ç”¨è€…å•é¡Œçš„ç›¸é—œæ€§é€²è¡Œé‡åŒ–è©•åˆ†

ä½¿ç”¨è€…å•é¡Œ: {query}

å¾…è©•ä¼°å…§å®¹ç‰‡æ®µ:
{chunk}

è©•åˆ†æ¨™æº–ï¼ˆç¸½åˆ†10åˆ†ï¼‰:
1. ç›´æ¥å›ç­”å•é¡Œ (0-3åˆ†)
   - 3åˆ†ï¼šå®Œå…¨ä¸”ç›´æ¥å›ç­”å•é¡Œæ ¸å¿ƒ
   - 2åˆ†ï¼šéƒ¨åˆ†å›ç­”å•é¡Œ
   - 1åˆ†ï¼šåƒ…æåŠå•é¡Œä½†æœªç›´æ¥å›ç­”
   - 0åˆ†ï¼šå®Œå…¨æœªå›ç­”å•é¡Œ

2. ç›¸é—œèƒŒæ™¯çŸ¥è­˜ (0-2åˆ†)
   - 2åˆ†ï¼šæä¾›è±å¯Œä¸”å¿…è¦çš„èƒŒæ™¯è³‡è¨Š
   - 1åˆ†ï¼šæä¾›ä¸€äº›ç›¸é—œèƒŒæ™¯
   - 0åˆ†ï¼šæœªæä¾›ç›¸é—œèƒŒæ™¯

3. å°ˆæ¥­æ€§å’Œæº–ç¢ºæ€§ (0-3åˆ†)
   - 3åˆ†ï¼šå…§å®¹å°ˆæ¥­ã€æº–ç¢ºä¸”æœ‰æ·±åº¦
   - 2åˆ†ï¼šå…§å®¹å¤§è‡´æº–ç¢ºä½†ç¼ºä¹æ·±åº¦
   - 1åˆ†ï¼šå…§å®¹æœ‰è¼•å¾®éŒ¯èª¤æˆ–éæ–¼ç°¡åŒ–
   - 0åˆ†ï¼šå…§å®¹æœ‰æ˜é¡¯éŒ¯èª¤æˆ–èª¤å°

4. å®Œæ•´æ€§ (0-2åˆ†)
   - 2åˆ†ï¼šå…§å®¹å®Œæ•´ï¼Œç„¡éœ€é¡å¤–è³‡è¨Š
   - 1åˆ†ï¼šå…§å®¹åŸºæœ¬å®Œæ•´ä½†æœ‰éºæ¼
   - 0åˆ†ï¼šå…§å®¹ç‰‡æ®µä¸”ä¸å®Œæ•´

æ€è€ƒæ­¥é©Ÿï¼š
1. ä»”ç´°é–±è®€ä½¿ç”¨è€…å•é¡Œï¼Œç¢ºå®šæ ¸å¿ƒéœ€æ±‚
2. é€æ¢è©•ä¼°å…§å®¹ç‰‡æ®µåœ¨å„æ¨™æº–ä¸‹çš„è¡¨ç¾
3. ç‚ºæ¯å€‹æ¨™æº–åˆ†é…é©ç•¶åˆ†æ•¸
4. è¨ˆç®—ç¸½åˆ†

åªè¼¸å‡ºæœ€çµ‚ç¸½åˆ†ï¼ˆ0-10çš„æ•´æ•¸ï¼‰ã€‚ä¸è¦åŒ…å«ä»»ä½•æ–‡å­—ã€è§£é‡‹æˆ–åˆ†æéç¨‹ã€‚
"""

    score_response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ç²¾ç¢ºçš„å…§å®¹è©•åˆ†å°ˆå®¶ã€‚ä½ å¿…é ˆéµå¾ªæŒ‡ç¤ºï¼Œåªå›å‚³ä¸€å€‹0-10ä¹‹é–“çš„æ•´æ•¸ä½œç‚ºè©•åˆ†çµæœï¼Œä¸èƒ½æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€æ¨™é»æˆ–èªªæ˜ã€‚é•åæ­¤è¦å‰‡å°‡å°è‡´è©•åˆ†ç³»çµ±å¤±æ•ˆã€‚"},
            {"role": "user", "content": score_prompt}
        ]
    )

    content = score_response.choices[0].message.content.strip()
    print(f"Chunk {index} raw score: {content}")

    try:
        # Try to convert response to integer
        score = int(content)
        # Ensure score is in 0-10 range
        return max(0, min(score, 10)), chunk
    except Exception as e:
        print(f"Error parsing score for chunk {index}: {e}")
        # If parsing fails, try to extract number from text
        number_match = re.search(r'\d+', content)
        if number_match:
            try:
                score = int(number_match.group())
                return max(0, min(score, 10)), chunk
            except:
                pass
        return 5, chunk  # Default to middle value


async def select_best_chunks_async(client, query, chunks, top_n=3):
    """Asynchronously score chunks and select the best ones."""
    print(f"Scoring {len(chunks)} chunks asynchronously...")
    
    # Use ThreadPoolExecutor for potentially blocking API calls
    scored_chunks = []
    with ThreadPoolExecutor(max_workers=min(10, len(chunks))) as executor:
        # Create task list
        futures = []
        for i, chunk in enumerate(chunks):
            future = executor.submit(score_chunk_sync, client, query, chunk, i)
            futures.append(future)
        
        # Wait for all tasks to complete
        for i, future in enumerate(futures):
            try:
                score, chunk = future.result()
                scored_chunks.append((score, chunk))
                print(f"Completed {i+1}/{len(chunks)} evaluations")
            except Exception as e:
                print(f"Error in chunk evaluation {i}: {e}")
                # If scoring fails, give low score but keep the chunk
                scored_chunks.append((1, chunks[i]))
    
    # Sort by score and take top_n
    scored_chunks.sort(key=lambda x: x[0], reverse=True)  # Sort from high to low
    top_chunks = [chunk for _, chunk in scored_chunks[:top_n]]
    
    return top_chunks


def summarize_chunks(client, query, chunks, language=None):
    """
    Summarize relevant chunks to generate a final answer.
    
    Args:
        client: OpenAI client
        query (str): The user's question
        chunks (list): List of relevant text segments
        language (str, optional): Preferred language code (e.g., 'en', 'zh-tw') according to user's language
            
    Returns:
        str: Summarized response in markdown format based on the chunks
    """
    # Handle empty chunks case
    if not chunks:
        return "No relevant information found to answer your query."
    
    # Prepare language instruction
    lang_instruction = ""
    if language:
        lang_instruction = f"Please respond in {language}. "
    
    # Create system prompt
    system_prompt = (
        f"{lang_instruction}Based on the user's question: '{query}', "
        "synthesize the following relevant information into a comprehensive answer. "
        "Maintain factual accuracy and cite information directly from the provided content."
    )
    
    try:
        # Combine chunks with proper separation and context
        formatted_chunks = "\n\n--- CHUNK START ---\n" + "\n--- CHUNK END ---\n\n--- CHUNK START ---\n".join(chunks) + "\n--- CHUNK END ---"
        
        # Make API call with error handling
        summarize_response = client.chat.completions.create(
            model="gpt-4o",
            temperature=0.2,  # Slight randomness for more natural responses while maintaining consistency
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": formatted_chunks}
            ],
            max_tokens=1024  # Set reasonable output length limit
        )
        
        return summarize_response.choices[0].message.content
    
    except Exception as e:
        # Log error and return fallback response
        print(f"Error in summarization: {str(e)}")
        return "Sorry, I encountered an issue while processing your request. Please try again."